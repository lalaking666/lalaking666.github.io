{"posts":[{"title":"JS逆向1: 有道翻译sign参数还原以及api返回数据解密","text":"发现有道翻译的api返回结果现在改成密文了，来试试看 1. 目标目标API: https://dict.youdao.com/webtranslate 请求方式为POST方式，表单提交的内容中字段sign为签名，并且接口返回内容为密文，看起来是base64。 2. 还原SIGN老规矩，还是先全局搜索sign，在文件app.f59abb30.js中发现可疑的地方。 调试一下，b函数接受的参数中t为毫秒时间戳、e的值固定:fsdsogkndfokasodnaso。b内部对e和t进行了拼接，后由p函数进行计算得出最终sign值。 p函数中拼接的字符串为:client=fanyideskweb&amp;mysticTime=1676711201946&amp;product=webfanyi&amp;key=fsdsogkndfokasodnaso。 然后对p函数传一个空值得出d41d8cd98f00b204e9800998ecf8427e，明显p函数是直接调用了md5算法。 小总结：sign的结果由当前的毫秒时间戳与一些固定字符串拼接后进行md5得出。3. 解密接口返回结果3.1 Hook javascript的json相关函数:1234567891011const my_stringify= JSON.stringify;JSON.stringify = function (params){ console.log(&quot;xxx&quot;,params); return my_stringify(params); };const my_parse = JSON.parse;JSON.parse = function (params){ console.log(&quot;xxx&quot;,params); return my_parse(params); }; 在console语句上打debugger查看调用栈后找到解密位置： 这里注意关键函数createDecipheriv这是nodejs中用于对称加密算法解密相关的，其中第一个参数为加密算法类型，第二值参数为加密的key，因为这里是aes的cbc模式所以第三个值应该为iv值。 3.2 用python还原:12345678910111213141516171819202122from Crypto.Cipher import AESfrom Crypto.Hash import MD5import base64from Crypto.Util.Padding import pad, unpaddef test_decrypt(decrypt_str): key = &quot;key&quot; iv = &quot;iv&quot; key = MD5.new(key.encode('utf8')).digest() iv = MD5.new(iv.encode('utf8')).digest() aes = AES.new(key, mode=AES.MODE_CBC, iv=iv) # decrypt_res = pad(decrypt_str.encode('utf8'), 16) # c = aes.decrypt(base64.b64decode(decrypt_str)) c = aes.decrypt(base64.urlsafe_b64decode(decrypt_str)) return unpad(c, AES.block_size).decode('utf8')if __name__ == &quot;__main__&quot;: decrypt_str = &quot;_jsUyA02rwkOJ4enKX7c4dhd7CjvGkcKfbRx0BjNGW_hwjE_2PiqPiM3vVr4Y05h0zY-PcyS28K16rKbb8q0Ma-qFiwk4jcdmZC34DcQsRF7kehjfLzArYdnCq-e09t-zhPi6UAlDgqVJeO_JGDg1veCbE-PBw_YawOjiZ7AEEcXJ9HvpazfDznmVv2cGhEUzn6npkrdLdHpe23yeNiUTOmUIsZVmmBQmv4vEoY_N3c=&quot; c = test_decrypt(decrypt_str) print(c) 3.3 小细节这里有一个小细节，接口返回的base64值并不是一个标准的base64值，而是一个变种：将Base64中的+和/改为了在url中不会被编码的_和/。在python解码base64时需要用urlsafe_b64decode函数。 4. 总结这个网站还是很简单的，只是base64解码那需要注意一下，作者在这卡了好久。","link":"/posts/78f06c31.html"},{"title":"flask-restful编写上传图片api","text":"Flask-RESTful是用于快速构建REST API的Flask扩展。我最近在使用Flask-Restful + Vue.js写一个轻量博客时有一个前端后端上传图片的需求。在Flask-Restful的官方文档中并没有相关的内容。下面是我谷歌查找资料的总结。 引入FileStorageflask-restful的参数解析中并没有文件类型，需要引入werkzeug.datastructures.FileStorage作为参数解析中的类型。上传图片的资源api可以这样编写： 123456789101112class UploadImg(Resource): def __init__(self): # 创建一个新的解析器 self.parser = reqparse.RequestParser() # 增加imgFile参数，用来解析前端传来的图片。 self.parser.add_argument('imgFile', required=True, type=FileStorage,location='files',help=&quot;imgFile is wrong.&quot;) def post(self): img_file = self.parser.parse_args().get('imgFile') # 保存图片 img_file.save(img_file.filename) return 'ok', 201 FileStorage这个类有很多的内置方法，这里使用了save方法保存了图片，save方法接受两个参数源码里面说明如下：dst指定保存文件的name. 12345def save(self, dst, buffer_size=16384): :param dst: a filename, :class:`os.PathLike`, or open file object to write to. :param buffer_size: Passed as the ``length`` parameter of :func:`shutil.copyfileobj`. 完整代码12345678910111213141516171819202122232425#!/usr/bin/env python# encoding: utf-8from flask_restful import reqparse, Resource, Apifrom werkzeug.datastructures import FileStoragefrom flask import Flaskclass UploadImg(Resource): def __init__(self): # 创建一个新的解析器 self.parser = reqparse.RequestParser() # 增加imgFile参数，用来解析前端传来的图片。 self.parser.add_argument('imgFile', required=True, type=FileStorage,location='files',help=&quot;imgFile is wrong.&quot;) def post(self): img_file = self.parser.parse_args().get('imgFile') img_file.save(img_file.filename) return 'ok', 201if __name__ == '__main__': app = Flask(__name__) api = Api(app) api.add_resource(UploadImg, '/uploadimg') app.run()","link":"/posts/9e5a0e91.html"},{"title":"python+requests爬虫澎湃新闻某条新闻下的评论","text":"如题例如抓取这个文章下的所有评论：链接。首先，列出需要抓取的数据： 新闻标题 新闻发布日期 评论者的昵称 评论的内容 分析网页请求找到需要的数据看下图，在网页的第一个请求里面已经包括了1，2两条的数据。接着在浏览器中向下滑动新闻网页加载评论，同时关注控制台，注意搜索框里的load，如下图浏览器会不断的发送请求给服务器，在这个请求的相应里面就包含了需要的3，4条(评论，评论者的用户名)数据。看一下这个请求的url有一堆的请求参数尝试精简下最后得到这样的urlhttps://www.thepaper.cn/load_moreFloorComment.jsp?contid=4489661startId=24750775。在这个url里面只有两个参数，第一个是新闻的id，第二是评论页的id。有了这个url就可以根据不同的startid构造出评论的url最终的抓到所有的评论信息。 怎么找到不同的startid？同样是上面那个图，在新标签打开对应的请求，看一下html源码，在第一条评论div里面有一个startid=’24745735’。记住这个值，再回去看一下第二条请求评论的url，发现最后的startid值就是第一条请求评论的url里面的startid值。就是这个样子：至此，所有的数据理论上来说都可以找到了。剩下的就是写代码了。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#!/usr/bin/env python# -*- coding: utf-8 -*-# ************************************************************************# *# * @file:penpai.py# * @author:kanhui# * date:2019-09-22 11:40:30# * @version 3.7.3# *# ************************************************************************import requestsimport refrom lxml import etreeimport jsonclass PengPaiSpider(): ''' 给定一个澎湃新闻的url爬取其下的评论信息 例如：https://www.thepaper.cn/newsDetail_forward_1292455 ''' def __init__(self): print('input url:') # 新闻url地址 self.url = input() # 用来判断是否到达最后一页，在请求评论页面时的第一条评论里有一个startId参数如果为0则表示没有下一页了 self.next_id = '' # 用来存储爬到的数据 self.item = {} # 请求头 self.header = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' } # 尝试获取contid 如果获取不到则设置为0程序结束 self.contid = re.findall(r'forward_(\\d*)', self.url)[0] if len( re.findall(r'forward_(\\d*)', self.url)) is not 0 else 0 self.comment_url = 'https://www.thepaper.cn/load_moreFloorComment.jsp?contid={}&amp;startId={}' if self.contid == 0: print('获取contid失败，检查url!!') def parse(self, url): '''发送请求函数''' res = requests.get(url) # 调试程序 print(url) return res.text def start(self): # 获取新闻信息,并获取评论条数 self.get_new_data() while self.next_id is not 0: res = self.parse(self.comment_url.format(self.contid,self.next_id)) # 根据res内容提取数据 self.handle(res) # 存储单条评论 self.save_comment() def handle(self, res): root = etree.HTML(res) comments = root.xpath( &quot;//div[@class='comment_que']//div[@class='ansright_cont']/a/text()&quot; ) usernames = root.xpath('.//h3/a/text()') self.item = [{ 'Nickname': username, 'content': comment } for username, comment in zip(usernames, comments)] # 更新下一个url # new_next_id = int(root.xpath('//div[@class=&quot;comment_que&quot;]')[0].xpath('./div')[1].xpath('@startid')[0]) new_next_id = int(re.findall(r'startId=&quot;(.*?)&quot;', res)[0]) # 老师给的url好像有点小问题，需要减14才能跳转到下一页，否则会在前两页已知循环 if self.next_id == new_next_id: self.next_id = new_next_id - 14 else: self.next_id = new_next_id def get_new_data(self): '''用来获取新闻的信息''' res = self.parse(self.url) root = etree.HTML(res) self.item['链接'] = self.url tmp = root.xpath(&quot;//h1[@class='news_title']/text()&quot;) self.item['标题'] = tmp[0] if len(tmp) is not 0 else '' tmp = root.xpath(&quot;//h2[@id='comm_span']/span/text()&quot;) if len(tmp) is not 0: self.comment_count = re.findall(r'\\（(.*)\\）', tmp[0])[0] if self.comment_count.isdigit(): self.item['评论数'] = self.comment_count else:# 处理是3.2k这种情况 self.comment_count = float(self.comment_count[:-1]) * 1000 self.item['评论数'] = self.comment_count tmp = root.xpath('//div[@class=&quot;news_about&quot;]/p/text()') if len(tmp) == 3: # 用正则表达式获取时间 post_time = re.findall(r'(\\d{4}-\\d{2}-\\d{2})', tmp[1])[0] if len( re.findall(r'(\\d{4}-\\d{2}-\\d{2})', tmp[1])) is not 0 else '' self.item['时间'] = post_time self.save_comment() def save_comment(self): with open(self.contid + '.json', 'a') as f: f.write(json.dumps(self.item, ensure_ascii=False, indent=4))if __name__ == '__main__': my_spider = PengPaiSpider() my_spider.start() print('完成！')","link":"/posts/ccc2cbf8.html"},{"title":"Picgo搭建Github图床使用Jsdelivr的正确姿势","text":"记录一下使用github + picgo搭建图床的流程 由于一些喜闻乐见的原因，github在我们国家的访问速度并不流畅，因此在使用github搭建图床时推荐使用jsdelivr进行cdn加速，根据网络上的大部分教程都是在设定仓库名时进行cdn设置：https://cdn.jsdelivr.net/gh/[用户名]/[仓库名]。如图： 我尝试了很多次，这种方法并不能成功。查看picgo错误日志： StatusCodeError: 404 - {&quot;message&quot;:&quot;Not Found&quot;,&quot;documentation_url&quot;:&quot;https://docs.github.com/rest/reference/repos#create-or-update-file-contents&quot;} 这个错误的大致意思就是找不到指定的url链接。 解决这个问题只需要在设定自定义域名的位置添加cdn加速就可以了，如图: 自定义域名处填写https://cdn.jsdelivr.net/gh/huihuidehui/github-pic@main此处的main要和设定的分支名保持一致。 最终实现效果在typora中编写markdown文档，进行图片插入时会自动调用picgo将图片上传到github图床，截图后在typora中粘贴自动上传到图床体验相当丝滑。","link":"/posts/a3dc8a98.html"},{"title":"python-supper方法用法-在子类中调用父类方法","text":"python-supper方法用法-在子类中调用父类方法 Python中使用super方法实现在子类中调用父类方法在python中新创建的子类会自动继承父类的所有信息，同时子类中重写某个方法也可以覆盖父类原有的方法。例如：在子类中重新定义__init__方法，那么子类中的__init__方法将会覆盖掉从父类继承的__init__.例如下面的代码： 12345678910111213141516171819class Person(): def __init__(self,name,age): self.name = name self.age = age print(&quot;{} is {} years old.&quot;.format(self.name,self.age))class Student(Person): def __init__(self,name,age,school): self.name = name self.age = age self.school = school print(&quot;{} is {} years old and comes from {} university.&quot;.format(name,age,school))if __name__ == '__main__': p = Person('DaMing',28) s = Student('XiaoMing',18,'bbb') 运行结果是这样的，可以看到在Student这个类中__init__已经覆盖了它的父类中的__init__方法。 12DaMing is 28 years old.XiaoMing is 18 years old and comes from bbb university. super方法的使用那如果想要在子类中调用父类中的初始方法应该怎么办呢？可以直接想到的解决方法就是我直接在子类中用父类的名称去调用它的方法不就好了，比如说在Student类中直接写Person.__init__()，这样做的确可行但是如果哪天修改了父类的名称那子类中对应的也需要做相应的修改不便于代码的维护。为了解决这个问题就要用到supper()这个方法了。我们改写一下Student这个类。 12345class Student(Person): def __init__(self,name,age,school): super().__init__(name,age) self.school = school print(&quot;{} is {} years old and comes from {} 上面的第3行代码super().__init__(name,age)实际上做了这样几件事情。 通过super()方法获取了父类person的定义。 子类中的__init__调用了Person.__init__()方法。它会自动的将self参数传递给父类，或者也可以指定参数super(cls,inst),cls为类，inst为类实例，然后传入其余参数即可。","link":"/posts/eb623950.html"},{"title":"Python多线程下单例模式实现","text":"单例模式算是在python用的比较多的设计模式，比较典型的应用场景包括：应用配置、日志文件、数据库调用…其特点是在整个系统周期仅会初始化一次，因此可以减少内存占用空间以及避免多次实例化。比如类AppConfig会在整个系统多次进行调用，如果每次调用都进行实例化，无疑会拖慢系统运行效率。 关于单例模式实现， 网上有很多花里胡哨的方法，这里介绍一种比较常见的方法。使用__new__方法实现。 基本原理python中实例化一个类时会在调用__init__方法前调用__new__方法，其接受的第一个参数是类本身，其余参数和__init__方法保持一致。因此可以在类中定义一个_instantce变量，在第一个进行示例化后将实例化的类保存到_instance中，之后实例化时只需判定_instance变量是否为空即可。 以实现一个全局计数器类为例一个最简单的版本，这种实现方法会多次调用__init__函数，并且存在多线程重复实例化问题。 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-class GlobalCount(object): # 用来保存实例化后的类实例 _instance = None def __new__(cls, *args, **kwargs): if cls._instance: print(&quot;跳过实例化&quot;) return cls._instance else: cls._instance = super(GlobalCount, cls).__new__(cls) return cls._instance def __init__(self): self._count = 0 @property def count(self): return self._count @count.setter def count(self, value): self._count = valueif __name__ == &quot;__main__&quot;: for i in range(10): gc = GlobalCount() gc.count = gc.count + 1 print(gc.count) 运行这段代码结果如下： 123456789101112131415161718191跳过实例化1跳过实例化1跳过实例化1跳过实例化1跳过实例化1跳过实例化1跳过实例化1跳过实例化1跳过实例化1 可以看到，虽然避免了重复实例化的问题，但并没有达到我们想要的效果。 避免多次调用__init__函数只需要在添加一个类变量:_init_flag作为标志位即可。在__init__方法内部将初始化相关的代码移动到if语句内，并在完成后将_init_flag设为False即可。 123456789101112class GlobalCount(object): # 用来保存实例化后的类实例 _instance = None _init_flag = True def __new__(cls, *args, **kwargs): if cls._instance: print(&quot;跳过实例化&quot;) return cls._instance else: cls._instance = super(GlobalCount, cls).__new__(cls) return cls._instance... 保证多线程安全继续完善GlobalCount类，目前在单线程下已经可以完美的工作了，但在多线程下有可能会存在多个线程同时进行实例化。可以使用threading包中的lock方法进行加锁处理。 123456789101112131415import threadingclass GlobalCount(object): # 用来保存实例化后的类实例 _instance = None _init_flag = True _instance_lock = threading.Lock() def __new__(cls, *args, **kwargs): with GlobalCount._instance_lock: if cls._instance: print(&quot;跳过实例化&quot;) return cls._instance else: cls._instance = super(GlobalCount, cls).__new__(cls) return cls._instance 总结总结一下，为了实现GlobalCount类，我们用到了__new__方法、property语法糖以及threading。","link":"/posts/d673f77c.html"},{"title":"python爬取知乎某个问题下所有图片","text":"python爬取知乎某个问题下所有图片.比方说 钓鱼贴 ??? 例如这个问题 1. 思路 在chorme浏览器控制台过滤一下answer关键字，在这个请求中api服务器返回了一个json格式的数据。在这个响应中data字段包含了这个问题下的三个回答，每个回答中的content字段包含了这个回答中所有图片的url。同时paging根据字段中的is_end和is_start可以判断出这是否是最后一页；paging字段中还有下一页的url. 现在问题就很简单了，我们只需要请求第一页并使用正则表达式提取出回答中图片的链接保存下来，接着根据响应中的paging的next的url依次请求下去即可。 2. 代码部分main.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# -*- coding: utf-8 -*-import jsonimport reimport grequestsimport threadingimport osimport click# import sys# sys.setrecursionlimit(1000000) # 例如这里设置为一百万BASE_URL = &quot;https://www.zhihu.com/api/v4/questions/{}/answers?include=data%5B*%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B*%5D.mark_infos%5B*%5D.url%3Bdata%5B*%5D.author.follower_count%2Cbadge%5B*%5D.topics&amp;limit={}&quot;IMG_BASE_URL = &quot;https://pic3.zhimg.com{}&quot;class ZhSpider(object): def __init__(self, question_id, min_voted_num): &quot;&quot;&quot; :param question_id: 问题ID，列表形式 :param min_voted_num: 将会过滤掉点赞数小于这个值得回答 &quot;&quot;&quot; self.min_voted_num = min_voted_num self.question_url = BASE_URL.format(question_id, 3) self.headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) &quot;, &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;, &quot;Accept-Language&quot;: &quot;en-us&quot;, &quot;Connection&quot;: &quot;keep-alive&quot;, &quot;Accept-Charset&quot;: &quot;GB2312,utf-8;q=0.7,*;q=0.7&quot;} def start(self): &quot;&quot;&quot; 启动爬虫 &quot;&quot;&quot; # 创建images目录 if not os.path.exists('images'): os.mkdir('images') print(&quot;已创建目录 'images', 所有的图片将会保存在该目录下&quot;) print('start') response = self.get(self.question_url) next_page_answer_url = self.parse(response) while next_page_answer_url is not None: response = self.get(next_page_answer_url) next_page_answer_url = self.parse(response) print(&quot;Done!&quot;) def get(self, url): &quot;&quot;&quot; 发送get请求，返回请求响应 &quot;&quot;&quot; rs = set([grequests.get(url, headers=self.headers)]) response = grequests.map(rs)[0] return response def parse(self, response): print(&quot;开始新的一页&quot;) print(&quot;开始处理响应...&quot;) dic = json.loads(response.text) # 此相应中的回答,过滤掉点赞数小于800的回答 answers_list = [ answer for answer in dic.get('data') if answer.get('voteup_count') &gt; self.min_voted_num ] if len(answers_list) is not 0: # 知乎的回答是按照点赞数来排序的，点赞数高的排在前面。所有只要爬到点赞数都不大于指定值的第一页就可以停下来了。 for answer in answers_list: answer_html_content = answer.get('content') # 正则选出回答中的img url incomplete_urls = re.findall( r'https:\\/\\/[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?', answer_html_content) # 使用集合去除重复图片 tmp_img_urls = set([later_url for _, later_url in incomplete_urls]) # 过滤高清大图 img_urls = [] for img_url in tmp_img_urls: if img_url[-5] == 'r': img_urls.append(IMG_BASE_URL.format(img_url)) print(&quot;开始下载图片:{}张.&quot;.format(len(img_urls))) rs = (grequests.get(u, headers=self.headers, timeout=5) for u in img_urls) res = grequests.map(rs) # 过滤掉404响应 res = [i for i in res if i.status_code == 200] print(&quot;开始保存图片: {}张.&quot;.format(len(res))) self.save_imgs(res) # 增加一个线程保存图片 # save_img_t = threading.Thread(target=self.save_imgs, args=(res,)) # print(&quot;开始保存图片: {}张&quot;.format(len(res))) # save_img_t.start() else: # 终止爬虫 return None if not dic.get('paging').get('is_end'): next_url = dic.get('paging').get('next') return next_url else: return None def save_imgs(self, data): &quot;&quot;&quot; :return: &quot;&quot;&quot; for i in data: with open('images/' + i.request.url[-18:], 'wb') as f: f.write(i.content) print(&quot;成功保存图片：{}张.&quot;.format(len(data)))@click.command()@click.option('--question', default=&quot;296631231&quot;, help=&quot;问题id&quot;, type=str)@click.option('--votenum', default=800, help=&quot;最小点赞数,将会过滤掉点赞数小于该值得回答&quot;, type=int)def start(question, votenum): &quot;&quot;&quot; :param answers: :param votenum: :return: &quot;&quot;&quot; # print(question, votenum) zh_spider = ZhSpider(question, votenum) zh_spider.start()if __name__ == &quot;__main__&quot;: start() 4.展示一下结果 5.最后代码已上传至github","link":"/posts/4fcb5fd7.html"},{"title":"scrapy爬虫苏宁易购图书信息","text":"在刚刚入门scrapy后尝试写了一个小的爬虫，目标是爬取苏宁易购下的所有的图书信息. 就是下面的每个大分类下的所有小分类的所有所售图书信息。我所抓取的数据有每个图书所在的大分类、小分类、作者、出版社、价格（这个数据的获取稍微麻烦一点）。 第一步： 分析网页首先需要找到我们要爬取的数据的位置。以及分析怎么在python中提取我们需要的数据。 1.1 找到要爬取的数据位置分类信息 打开谷歌浏览器进入https://book.suning.com/按F12进入控制台，进入network选项后，刷新一下。如下图所示，在第一个请求获取的响应中已经是包含了所有分类的url地址了。所以可以把此url当作初始url。 图书信息 接下寻找图书的详细信息所在的位置。随便打开一个图书的链接。进入控制台查看第一个请求所获得的响应。如下图所示可以看到我们需要获取的信息除了价格外全都有了。 价格信息 现在只剩下一个价格信息了，在图书的详情页面打开控制台找到nsp开头的那个请求，价格就在这个请求里面。如下图。查看这个请求的url地址可以发现有两个参数是不确定的https://pas.suning.com/nspcsale_0_000000010227414878_000000010227414878_0070183717_120_536_5360101_502282_1000235_9232_11809_Z001___R9011546_1.0___.html其中的10227414878，701837171是不确定的。这两个参数的是在图书的详情页面的链接中是有的。直接正则提取就可以（比如这个https://product.suning.com/0070183717/10227414878.html?srcpoint=pindao_book_27582454_prod01）后面的R9011546是可以去掉的。通过多观察几个图书的请求价格的url链接可以发现 1120_536_5360101_502282_1000235_9232_11809_Z001_ 是固定的。所以最终请求价格的url地址https://pas.suning.com/nspcsale_0_{}_{}_{}_120_536_5360101_502282_1000235_9232_11809_Z001_1.0___.html&quot;。 {}部分在代码中会用format函数格式化。 # 1.2 分析如何获取数据 正则 or xpath? 找到了需要的数据所在的位置后，下一步就是从网页中提取数据。 ## 提取分类的url 可以在谷歌浏览器中使用xpath helper尝试使用xpath定位到每个小分类的位置。并获取分类的url。用代码实现如下： 1234567# 获取每个大分类下的所有小分类并遍历获取每个小分类对应的url地址categories_big = response.xpath(&quot;//div[@class='menu-list']/div&quot;) # 这里的reponse是scrapy默认的parse函数里的参数 for category_big in categories_big: detail_categories = category_big.xpath(&quot;./dl/dd/a&quot;) for detail_category in detail_categories: # 得到小分类的url地址 category_detail_url = detail_category.xpath(&quot;./@href&quot;).extract_first() 获取图书信息 获取图书信息这一步可以使用正则表达式来完成简单粗暴！！！。 1234567891011# 首先用xpath的获取1.1图书信息图中的meta标签的值。 res = response.xpath(&quot;//meta[@property='og:description']/@content&quot;).extract_first() # 下面的代码是直接正则拿到数据。 item = {} item['book_name'] = re.findall(r&quot;《(.*)》，作者&quot;, res)[0] if len(re.findall(r&quot;《(.*)》，作者&quot;, res)) else '' # item['author'] = re.findall(r&quot;作者：(.*)著&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)著&quot;, res)) else '' item['author'] = re.findall(r&quot;作者：(.*)，.*出版&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)，.*出版&quot;, res)) else '' item['publisher'] = re.findall(r&quot;作者：.*，(.*)出版&quot;, res)[0] if len(re.findall(r&quot;作者：.*，(.*)出版&quot;, res)) else '' item['publisher'] = item['publisher'] + '出版社' item['big_category'] = re.findall(r&quot;品类：(.*)&gt;&quot;, res)[0] if len(re.findall(r&quot;品类：(.*)&gt;&quot;, res)) else '' item['detail_category'] = re.findall(r&quot;&gt;(.*)，以及&quot;, res)[0] if len(re.findall(r&quot;&gt;(.*)，以及&quot;, res)) else '' 价格 前面已经得到了获取价格的url地址直接请求就可以，返回的数据是直接正则拿到价格。这个简单就不详细说了。主要是这个数据还需要另外请求一个url就比较麻烦。。。 第二步： 开始编写代码2.1 新建scrapy项目 这里我假设你已经安装好了python3并且装了scrapy模块。如果没有，百度一下？教程很多！ 选择一个合适的目录打开终端 123scrapy startproject suningcd suningscrapy genspider book suning.com 2.2 项目结构12345678910111213141516171819202122(13-爬虫) kanhui@kanhui-Mac ~/D/2/2/1/suning&gt; tree.├── scrapy.cfg└── suning ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── items.cpython-37.pyc │ ├── pipelines.cpython-37.pyc │ └── settings.cpython-37.pyc ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py ├── setup.py └── spiders ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ └── book.cpython-37.pyc └── book.py 2.3详细代码book.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#!/usr/bin/env python# -*- coding: utf-8 -*-# ************************************************************************# *# * @file:book.py# * @author:kanhui# * date:2019-09-01 14:01:05# * @version vim 8# *# ************************************************************************import scrapyimport refrom suning.items import SuningItemfrom copy import deepcopyclass BookSpider(scrapy.Spider): name = 'book' allowed_domains = ['suning.com'] start_urls = ['https://book.suning.com/?safp=d488778a.46602.crumbs.2'] def parse(self, response): '''获取图书分类地址，进行分组''' categories_big = response.xpath(&quot;//div[@class='menu-list']/div&quot;) for category_big in categories_big: detail_categories = category_big.xpath(&quot;./dl/dd/a&quot;) for detail_category in detail_categories: category_detail_url = detail_category.xpath(&quot;./@href&quot;).extract_first() # yield一个request对象 print(&quot;爬取分类&quot;) yield scrapy.Request( category_detail_url, callback=self.parse2 ) print(&quot;结束一个分类的爬去&quot;) def parse2(self, response): # 获取当前页面的每个图书的信息 results = response.xpath(&quot;//div[@class='res-info']&quot;) for res in results: item = {} book_detail_url = res.xpath(&quot;./p[@class='sell-point']/a/@href&quot;).extract_first() ids = re.findall(r&quot;com/(.*)/(.*).html&quot;, book_detail_url) if len(ids): item['shop_id'], item['prdid'] = ids[0] else: item['shop_id'] = item['prdid'] = '' # yield一个request对象 yield scrapy.Request( 'https:' + book_detail_url, callback=self.parse3, meta=item ) self.base_url = &quot;https://list.suning.com/{}-{}.html&quot; # cur_url = response.xpath(&quot;//a[@class='']/@href&quot;).extract_first() cur_url = response.url # 当前的页码数恰好和下一页的url地址中的也码数相同。 prefix, cur_page = re.findall(r&quot;com/(.*)-(.*).html&quot;, cur_url)[0] if int(cur_page) &lt; 100: next_url = self.base_url.format(prefix, str(int(cur_page) + 1)) print(&quot;爬取{}页。\\n&quot;.format(str(cur_page))) yield scrapy.Request( next_url, callback=self.parse2 ) def parse3(self, response): '''书籍详情页面''' res = response.xpath(&quot;//meta[@property='og:description']/@content&quot;).extract_first() item = {} item['book_name'] = re.findall(r&quot;《(.*)》，作者&quot;, res)[0] if len(re.findall(r&quot;《(.*)》，作者&quot;, res)) else '' # item['author'] = re.findall(r&quot;作者：(.*)著&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)著&quot;, res)) else '' item['author'] = re.findall(r&quot;作者：(.*)，.*出版&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)，.*出版&quot;, res)) else '' item['publisher'] = re.findall(r&quot;作者：.*，(.*)出版&quot;, res)[0] if len(re.findall(r&quot;作者：.*，(.*)出版&quot;, res)) else '' item['publisher'] = item['publisher'] + '出版社' item['big_category'] = re.findall(r&quot;品类：(.*)&gt;&quot;, res)[0] if len(re.findall(r&quot;品类：(.*)&gt;&quot;, res)) else '' item['detail_category'] = re.findall(r&quot;&gt;(.*)，以及&quot;, res)[0] if len(re.findall(r&quot;&gt;(.*)，以及&quot;, res)) else '' ids = response.meta shop_id = ids.get('shop_id') prdid = ids.get('prdid') prdid = &quot;0&quot; * (18 - len(prdid)) + prdid url = &quot;https://pas.suning.com/nspcsale_0_{}_{}_{}_120_536_5360101_502282_1000235_9232_11809_Z001_1.0___.html&quot;.format( prdid, prdid, shop_id) yield scrapy.Request( url, self.get_price, meta=deepcopy(item) ) def get_price(self, response): item = response.meta price = re.findall(r'&quot;promotionPrice&quot;:&quot;(.*)&quot;,&quot;bookPrice&quot;', response.text)[0] if len( re.findall(r'&quot;promotionPrice&quot;:&quot;(.*)&quot;,&quot;bookPrice&quot;', response.text)) else 0 item['price'] = float(price) book_item = SuningItem() book_item['book_name'] = item.get('book_name') book_item['author'] = item.get('author') book_item['publisher'] = item.get('publisher') book_item['big_category'] = item.get('big_category') book_item['detail_category'] = item.get('detail_category') book_item['price'] = item.get('price') yield book_item item.py 12345678910#!/usr/bin/env pythonclass SuningItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() big_category = scrapy.Field() detail_category = scrapy.Field() book_name = scrapy.Field() price = scrapy.Field() publisher = scrapy.Field() author = scrapy.Field() piplines.py 123456789101112from pymongo import MongoClientclient = MongoClient()collections = client['suning']['book1']class SuningPipeline(object): def process_item(self, item, spider): collections.insert(dict(item)) print(str(item)) return item 注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。最后 程序运行结果如下图，我运行了大概30分钟抓到了30000多条数据。","link":"/posts/46548bc2.html"},{"title":"使用flask-httpauth给你的flask-restful-api加入请求认证","text":"最近在用vue.js和flask-restful开发一个前后端分离的个人博客，其中有一个接口article用于根据文章ID对文章进行查询(get请求)、修改(post请求)等操作。显然这个接口只有查询操作能对所有用户开放而修改操作只对管理员开放。这时候就需要对aritcle接口进行请求认证，由于Restful api并不保存状态，因此也就无法使用Flask-login依赖Cookie以及session来实现认证。所有这里我使用了另一个扩展Flask-HTTPAuth 1. 重写前的接口先来看一下重写前的接口下面是一个demo。article接口支持get、post请求，分别用于查询文章信息和修改文章信息， 用列表来存储文章数据使用列表嵌套字典的形式，存储两个文章，每个文章包含了两个字段id用于查询文章和content存储文章的内容。 1234567891011from flask_restful import reqparse, Resourcearticles = [ { 'id': 1, 'content': &quot;test article 1&quot; }, { 'id': 2, 'content': &quot;test article 2&quot; }] 使用flask-restful编写article接口使用flask-restful编写article接口，接受两种请求get和post,分别用于查询文章和修改文章 1234567891011121314151617181920212223242526272829class Article(Resource): def __init__(self): self.parser = reqparse.RequestParser() self.parser.add_argument('id', type=int) self.parser.add_argument('content', type=str) def get(self): # 从get请求中获取文章id值 article_id = self.parser.parse_args().get('id') # 根据id从列表中查找文章 for article in articles: if article.get('id') is article_id: # 返回查找到的数据 return article, 200 def post(self): # img_file = self.parser.parse_args().get('imgFile') # img_file.save(img_file.filename) # 从请求中获取content值和id值 article_id = self.parser.parse_args().get('id') content = self.parser.parse_args().get('content') # 查找文章 for article in articles: if article.get('id') is article_id: res = article # 修改文章 res['content'] = content return res, 201 2. 使用flask-httpauth添加请求认证我们不希望article这个接口的post请求对任何人都开放。通过引入flask-httpauth对article接口的post请求加入请求认证。Flask-HTTPAuth是一个简单的扩展，它简化了HTTP身份验证与Flask路由的使用。其提供了多种不同的Auth方法，比如HTTPBasicAuth，HTTPTokenAuth，MultiAuth和HTTPDigestAuth。这个例子中我们使用基本的密码验证，同时 2.1 安装flask-httpauth在你的终端窗口执行: 1pip install Flask-HTTPAuth 创建flask-httpauth对象实象。需要注意，初始化实例时不需要传入app对象，也不需要调用auth.init_app(app)注入应用对象。 123from flask import Flaskfrom flask_httpauth import HTTPBasicAuth 2.2 使用HTTPBasicAuth使用我们先简单的使用HTTPBasicAuth进行用户和密码认证。创建HTTPBasicAuth实例对象。为了方便演示，我们把用户名和密码存入一个字典里面。 123456auth_basic = HTTPBasicAuth()# 单用户user = { 'username':&quot;amdin&quot;, 'password':'123456'} 2.2.1 对article 接口添加请求认证@auth_basic.login_required装饰器表示视图需要认证，在flask-restful编写的api中我们需要的在接口中添加类变量method_decorators。其值是一个字典形式。下面的代码表示Article接口的post和delete请求需要认证。 12345class Article(Resource): method_decorators = { 'post': [auth_basic.login_required], 'delete': [auth_basic.login_required] } 2.2.2 编写验证用户密码的回调函数@auth_basic.verify_password装饰器用于验证用户名和密码。在它所修饰的函数里面返回True或者是False. 123@auth_basic.verify_passworddef verify_password(username,password): return user['username'] == username and user['password'] == password 2.2.3 对接口进行测试使用curl命令对接口进行测试, 会得到Unauthorized Access表示没有权限。 1curl -d &quot;id=1&amp;content=modify article1&quot; -X POST http://127.0.0.1:5000/article 加入用户名和密码后则可以正常修改文章。 1curl -u admin:123456 -d &quot;id=1&amp;content=modify article1&quot; -X POST http://127.0.0.1:5000/article 2.2.4 错误处理在之前的例子中，如果未认证成功，服务端会返回401状态码及”Unauthorized Access”文本信息。可以通过重写错误处理方法，自定义返回的错误信息。用@auth_basic.error_handler装饰器来修饰错误处理方法： 1234from flask import make_response, jsonify@auth_basic.error_handlerdef auth_basic_unauthorized(): return make_response(jsonify({'res': 0, 'message': 'Unauthorized access'}), 401) 加入这个错误处理方法后，认证失败后服务器会返回JSON信息{res:0,message:Unauthorized access}. 2.2.5 使用Flask-Bcrypt在大多数情况下，数据库中的密码是以非明文的形式存储的。可以使用Flask-Bcrypt扩展对密码进行哈希后再存储到数据库中。官方文档在这里。 2.3 使用 HTTPTokenAuth大部分情况下我们并不是使用密码和用户名进行验证的，而是使用token来做验证。Flask-HTTPAuth中的HTTPTokenAuth可以完成token验证。与HTTPBasicAuth相似，它也提供了login_required装饰器来认证视图函数，error_handler装饰器来处理错误。不同的是它没有verify_password装饰器取而代之的是verify_token。在这里我是使用一个login接口用于登录操作，登录之后将会使用用户名和密码生成一个token，存储在flask的全局变量g中，以供token验证时调用。具体代码等有时间在写吧。。。 未完。。。","link":"/posts/1af3d3fa.html"},{"title":"分享Paragon NTFS for Mac 无限试用破解版","text":"大概在2019.年3月份的时候我刚给笔记本装上黑苹果时，我很苦恼的发现这个Mac os只能读ntfs格式的移动硬盘却写不了🌚. 提示：下面这个版本在本机10.15.3可正常使用. Paragon ntfs 15|ntfs for mac 破解版是一个底层的文件系统驱动程序,专门开发用来弥合Windows和Mac OS X之间的不兼容性，通过在Mac OS X系统下提供对任何版本的NTFS文件系统完全的读写访问服务来弥合这种不兼容性。为您轻松解决Mac不能识别Windows NTFS文件难题，让您简单自如读写NTFS外置存储文件。您无法在NTFS分区中创建、删除或者修改文件或文件夹，而仅仅只能读取。通过Paragon的 Mac OS X专用NTFS，您可以在Mac OS X下全读/写访问NTFS的任何分区。 1. 使用方法双击打开dmg包选择安装NTFS for mac.app之后一路下一步即可。 之后在这一步选择10天使用 在10天过期之后使用安装包里面的Trial Reset.app重置试用后就可以重新开始10天试用了. 棒👍 2. 出现Mac文件损坏处理方法打开Mac终端 输入以下代码回车sudo spctl --master-disable(终端是Mac系统的一个系统软件,在实用菜单文件夹中,找不到就用Mac系统搜索”终端”两个字)之后输入电脑密码打开所有来源即可运行！ 注意：在终端下输入密码不会显示，输入完成直接回车即可，如果输入错误会用英文提示错误请重试，再次输入正确的密码回车即可； 3. 某盘链接链接:https://pan.baidu.com/s/1gID7Ax-dl_5z76iehVOAGg 密码:arxx 如果链接失效了可以在下方评论提醒我更新。","link":"/posts/908a119d.html"},{"title":"李宏毅机器学习logistic-regression","text":"台大李宏毅机器学习课程第二次作业，logistic regression预测收入。这是我对这个作业的总结，方便日后复习。代码和训练数据已上传至github,戳我 知识准备2019.11.24号这个周末用了两天的时间终于做完了，期间参考的各种资料我会在文章最后列出。那完成这次作业至少需要具备下面的知识点。 微积分 梯度下降原理 pandas和numpy的基本使用 作业要求课程提供的作业要求文档.(其中的kaggle提交地址已失效,我在文章末尾提供了训练集和测试集的下载地址)。给定训练集和测试集，根据训练集建立模型判断测试集中的每个id对应的收入是否超过50K. 思路分析首先，很明显的二分类问题，根据在b站(对，就是b站)课堂上学到内容选择使用logistic regression(对数几率回归)来实现.数值优化使用梯度下降，当然其他方法也是可以的。 1. 引入对数几率预测函数先引入一个概念：广义线性模型。在西瓜书上的定义是这样的考虑单调可微函数$g(X)$令 $ y = g(w^Tx +b)$这样就得到了“广义线性模型(generalized linear model)“其中函数$g(X)$称为”联系函数(link function)“.显然，线性回归模型就是$g(X) = 1,x \\in R$ 。 现在回到问题中，我们需要使用线性模型解决一个二分类的问题，那么只需要找到一个单调可微的函数$g(X)$将分类任务的真实标记y与线性回归模型的预测值联系起来即可。这句不理解没关系，接着看下一段就明白了 线性回归模型中产生的预测值$z = w^Tx + b$这里的$z$是一个实值$z \\in R$。二分类问题中我们想要的预测值$y$是一个离散值0或者是1，即$y$的值是正例或者反例。那么我们只需要找一个函数 $ y = g(z) $ 它的输入值 $z$ 输出值是 $ y \\in [0,1] $ 。最理想的函数是”单位阶跃函数“。如下： $$g(z) = \\begin{cases} 0 &amp; z &lt; 0\\ 0.5&amp;z=0 \\ 1&amp;z&gt;0 \\end{cases} $$ 即若预测值$z$大于0判断为分类1，小于0则判断我分类2，预测值为临界值则可以任意判别。但是这个函数如下图它并不连续不能作为广义线性模型中的$g(X)$。于是我们希望找到一个函数一定程度上相似单位阶跃函数同时单调可微。对数几率函数就是这样一个函数： $$g(z) = \\frac{1}{1 + e^{-z}}$$ 对数几率函数与单位阶跃函数图像如下： 我们把这个函数 代入到广义线性模型中就得到了对数几率回归的预测函数 $$y = \\frac{1}{1+e^{-z}} = {1 \\over 1+ e^{-{(w^Tx+b)}}}$$。 1.1 解释对数几率的含义现在我们来解释一下“对数几率”这个词的含义。首先我们先把 $y = \\frac{1}{1+e^{-z}} $ 中的$z$提取出来就得到了 $ln{\\frac{y}{1-y}}=w^Tx+b$ 。其中如果把y看作为样本是正例的可能性，则$1-y$是其反例的可能性，这两者的比值 $ \\frac{y}{1-y} $就称为几率，反映了$x$作为正例的可能性。对几率取对数得到的 $ ln\\frac{y}{1-y} $就称为对数几率。 2.找到损失函数现在已经知道了对数几率回归的预测函数，只需要找到损失函数，然后用梯度下降法确定w和b的值就完成任务了。对数几率回归中损失函数并是线性回归中的平方误差损失，而是交叉熵(可以参考这篇文章);或者也可以使用对数似然推导(可以参考书籍-机器学习-周志华第三章部分)出来损失函数结果是一样的。交叉熵老师在视频里已经给出了公式： $$ L(w,b) = -[ylny^* + (1-y)ln(1-y^*)]$$ 这个损失函数通常称作为 对数损失 (logloss)，这里的对数底为自然对数 ，其中真实值 是有 0/1 两种情况，而推测值$ y^* $ 由于借助对数几率函数，其输出是介于0~1之间连续概率值。仔细查看，不难发现，当真实值 时，第一项为0，当真实值 时，第二项为0，所以，这个损失函数其实在每次计算时永远都只有一项在发挥作用，那这不就可以转换为分段函数了吗，分段的形式如下: 不难发现，当真实值 为1时，输出值 越接近1，则 越小，当真实值 为 0 时，输出值 越接近于0，则 越小 (可自己手画一下 函数的曲线)。该分段函数整合之后就是上面我们所列出的 logloss 损失函数的形式。 3.使用梯度下降优化求解我们确定了模型的损失函数，接下来就是根据这个损失函数，不断的优化模型的参数，从而获得拟合数据的最佳模型。损失函数如下： $$L(w,b) = -[ylny^* + (1-y)ln(1-y^*)], 注意y不是自变量，y^*=\\frac{1}{1+e^{-z}}, z = w^Tx + b$$ 最小化损失值便得到 $(w^*,b^*) = argmin_{w,b}(L(w,b))$ 梯度下降中的$(w,b)$的更新方式 $$w \\leftarrow w - \\alpha\\frac{\\partial L}{\\partial w}$$ $$b \\leftarrow b - \\alpha\\frac{\\partial L}{\\partial b}$$ 由微积分的链式求导可以得到 $$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y^*} \\frac{\\partial y^*}{\\partial z} \\frac{\\partial z}{\\partial w} = (y^* - y)x$$ $$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y^*} \\frac{\\partial y^*}{\\partial z} \\frac{\\partial z}{\\partial b}=y^*-y$$ 转化为矩阵运算： 为了简便运算我们在训练集上添加一个全为1的列，使偏置系数b更新的时候不受x值得影响，接着令$\\beta = (W,b)$那么参数更新就可以写为 $$\\beta = \\beta - \\alpha X^T(Y^* - Y)$$。 其中它们的shape如下： $$\\begin{cases} \\beta &amp; (n,1) \\ X &amp; (m,n) \\ Y^*,Y&amp;(m,1)\\end{cases}$$ 代码实现下面是代码实现的部分，其中数据处理部分占了很大一部分，核心的梯度下降部分代码如下： 1234567891011121314151617181920def gradient_secent(x_train, y_train): ''' args: x_train: m x n y_train: m x 1 return: wb ''' lr_rate, threshold = 1, 0.01 # 初始化学习速度和阈值 wb = np.ones((x_train.shape[1], 1)) # n x 1, 这里的wb就是刚刚的贝塔（就是那个和B很像的字母） g = np.zeros((x_train.shape[1], 1)) # 使用adagrad更新学习速率， y_predict = get_y_predict(wb, x_train) loss_value = get_loss_value(wb, y_train, y_predict) # 计算损失值 while True: wb, g = update_parameter(wb, lr_rate, x_train, y_train, y_predict, g) y_predict = get_y_predict(wb, x_train) pre_loss_value, loss_value = loss_value, get_loss_value( wb, y_train, y_predict) if abs(pre_loss_value - loss_value) &lt; threshold: break return wb 完整的代码部分如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182#!/usr/bin/env python# encoding: utf-8import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdef binary_value(column_data,cond): ''' args: column_data: 需要二值化的列 cond: 二值化的条件，符合该条件的设置为1否则为0 return: 返回处理好的数据 ''' column_data.mask(cond=cond,other=1,inplace=True) column_data.where(cond=column_data==1,other=0,inplace=True) #print(column_data) return column_data.astype('int64')def one_hot_scale(data): ''' 对数据进行one-hot编码，并进行缩放. args: data: 原始数据 return: 处理后的数据 ''' object_columns = [column for column in data.columns if data[column].dtype == 'object'] number_columns = [column for column in data.columns if data[column].dtype == 'int64' and column != 'income' and column != 'sex'] object_columns, number_columns = data[object_columns],data[number_columns] # 提取相应的列 # 对数据进行标准化缩放,缩放后值在0-1之间 number_columns = (number_columns - number_columns.min()) / (number_columns.max() - number_columns.min()) object_columns = pd.get_dummies(object_columns) data = pd.concat([object_columns, number_columns, data['sex'], data['income']],axis=1) return datadef data_pre_process(df_data,df_test_data): ''' 1. 进行数据预处理 2. 返回训练集和验证集 ''' df_data,df_test_data = df_data.fillna(0),df_test_data.fillna(0) # 对空值进行填充 # 对性别和income列处理 df_data['sex'] = binary_value(df_data['sex'].copy(deep=True),cond=df_data['sex']==' Male') df_test_data['sex'] = binary_value(df_test_data['sex'].copy(deep=True),cond=df_test_data['sex']==' Male') df_data['income'] = binary_value(df_data['income'].copy(deep=True),cond=df_data['income']==' &gt;50K') df_data = one_hot_scale(df_data) df_test_data= one_hot_scale(df_test_data) # 把训练集分为训练集和验证集 x_train = df_data.iloc[:int(len(df_data.index) * 0.80), :] x_valid = df_data.iloc[int(len(df_data.index) * 0.80):, :] y_train = np.array(x_train.iloc[:, -1:]) x_train = np.array(x_train.iloc[:, :-1]) x_train = np.c_[x_train, np.ones((x_train.shape[0], 1))] # 添加一个横为1的列，用于简化偏置系数b的更新 y_valid = np.array(x_valid.iloc[:, -1:]) x_valid = np.array(x_valid.iloc[:, :-1]) x_valid = np.c_[x_valid, np.ones((x_valid.shape[0], 1))] y_test = np.array(df_test_data.iloc[:,-1:]) x_test = np.array(df_test_data.iloc[:,:-1]) x_test = np.c_[x_test, np.ones((x_test.shape[0], 1))] # 添加一个横为1的列，用于简化偏置系数b的更新 return x_train, y_train, x_valid, y_valid,x_test,y_testdef gradient_secent(x_train, y_train): ''' args: x_train: m x n y_train: m x 1 return: wb ''' lr_rate, threshold = 1, 5 wb = np.ones((x_train.shape[1], 1)) # n x 1 g = np.zeros((x_train.shape[1], 1)) y_predict = get_y_predict(wb, x_train) loss_value = get_loss_value(wb, y_train, y_predict) # 计算损失值 loss_value_list = [] while True: loss_value_list.append(loss_value) wb, g = update_parameter(wb, lr_rate, x_train, y_train, y_predict, g) y_predict = get_y_predict(wb, x_train) pre_loss_value, loss_value = loss_value, get_loss_value(wb, y_train, y_predict) if abs(pre_loss_value - loss_value) &lt; threshold: break # 循环结束条件 plot_loss_value(loss_value_list) #画图 return wbdef update_parameter(wb, lr_rate, x_train, y_train, y_predict, g): ''' args: wb: n x 1 g: adagrad 里的 G, n x 1 lr_rate: 学习速率 y_train: 真实值，m x 1 y_predict: 预测值，m x 1 return: wb, g ''' descent = x_train.T.dot(y_predict - y_train) # n x 1 g = g + descent * descent lr_rates = lr_rate / (np.sqrt(g + 0.00000001)) # n x 1 wb = wb - lr_rates * descent return wb, gdef get_y_predict(wb, x_train): ''' args: wb: 参数，n x 1 x_train: 训练集，m x n return:计算预测值 y.shape = m x 1 ''' y = wb.T.dot(x_train.T) # 1 x n * n x m = 1 x m y = 1 / (np.exp(-1 * y) + 1) # 1 x m y = y.T # m x 1 return ydef get_loss_value(wb, y_train, y_predict): ''' args: wb: 参数，n x 1 y_train: 真实值，m x 1 y_predict: 预测值，m x 1 return: loss_value, 损失值 ''' res = y_train.T.dot(np.log(y_predict)) # 1 x m * m x 1 res = -1 * res tmp = (y_train - 1).T.dot(np.log(1 - y_predict)) res = res + tmp return res[0][0]def get_acc(wb, x, y): ''' args: wb: n x 1 x: m x n y: m x 1 return: 正确率 acc ''' y_predict = get_y_predict(wb, x) # m x 1 y_predict[y_predict &gt; 0.5] = 1 y_predict[y_predict &lt;= 0.5] = 0 acc = sum(y_predict == y)[0] / y.shape[0] return accdef test_data_process(df_data): ''' args: df_data: m x n return: x_test, y_test '''def main(): train_data = pd.read_csv('./data/train.csv') test_data = pd.read_csv('./data/test.csv') x_train, y_train, x_valid,y_valid,x_test,y_test = data_pre_process(train_data,test_data) wb = gradient_secent(x_train, y_train) valid_acc = get_acc(wb, x_valid, y_valid) print(&quot;valid accuracy:{}&quot;.format(valid_acc)) test_acc = get_acc(wb,x_test,y_test) print(&quot;tet accuracy:{}&quot;.format(test_acc)) plt.show()def plot_loss_value(loss_values): ''' 画出loss value的变化图 ''' plt.plot(range(len(loss_values)),loss_values,linewidth=4,color='g')if __name__ == '__main__': ''' 李宏毅机器学习第二次作业 1. 对数几率回归 2. 使用梯度下降寻找最优解 ''' print(&quot;正在执行中，可能需要耗费大量时间，如果超过10分钟未响应请手动关掉即可。&quot;) main() 运行截图： 可以看出在验证集和测试集中正确率还算看的过去。。。可能做了特征工程后正确率会高一点。 参考链接感谢下面的博主和作者。 《机器学习》——周志华 对数几率回归 —— Logistic Regression 数据集连接","link":"/posts/2a54212a.html"},{"title":"极验-3代一键通过模式-反混淆fullpage.9.1.9.cyhomb.js","text":"案例地址: aHR0cHM6Ly9kZW1vcy5nZWV0ZXN0LmNvbS9mdWxscGFnZS5odG1s 1. 分析整个请求流程先刷新一下页面，看一下整体流程，如图总共有4个关键请求。 1. register-fullpage 这个请求用于获取gt和challenge参数，后面会用到。 这个请求用于获取gt和challenge参数，后面会用到 2. gettype.php 看请求的名字是获取验证码的类型，请求时需要带着第一步的gt参数。 看请求的名字是获取验证码的类型，请求时需要带着第一步的gt参数。 3. get.php这个请求就有说法了，喜闻乐见的w值就在其请求参数内，除了w值外还有gt和challenge参数。其中gt和challenge参数是第一步register-fullpage请求中得到的，关键参数就是w值。这里如果通过全局搜索w字符串并没有什么有效的位置，而且这个请求不是xhr请求，没办法通过xhr断点断住，这里用initiator的进行追踪：最终可以定位到fullpage.9.1.9-cyhomb.js文件，看它样子就知道八九不离十就是它了。可看到这是经过混淆的js文件，接下来为了方便分析代码，我们可以通过ast来进行适当的还原混淆的代码，在正式开始反混淆之前可以先用v_jstool工具进行一次普通解混淆，这一步会对代码中的unicode字符串进行处理，以及一些基础的反混淆操作。 2. 使用ast进行反混淆这里是babel相关的库来进行解混淆，配合[https://astexplorer.net/](https://astexplorer.net/)网站食用。 2.1 介绍一下babel Babel 是一个 JavaScript 编译器​，Babel以及一个工具链，主要用于将采用 ECMAScript 2015+ 语法编写的代码转换为向后兼容的 JavaScript 语法，以便能够运行在当前和旧版本的浏览器或其他环境中。下面列出的是 Babel 能为你做的事情： 语法转换 通过 Polyfill 方式在目标环境中添加缺失的功能（通过引入第三方 polyfill 模块，例如 core-js） 源码转换（codemods） 这是babel文档中对于它的介绍，我们主要用到的是它的源码转换功能。举个栗子，通过babel我们可以将以下代码进行简化方便阅读。 12345var a,b,c = 1,2,3;// 转换后var a = 1;var b = 2;var c = 3; 123456789101112function add(x, y){ // 这两行是无用的干扰代码 var cc = ff.d; var dd = cc[0]; return x + y;}// 转换后function add(x,y){ return x + y;} 当然要完成这些操作，首先是要学习如何使用babel，个人认为上手一种新的技术最好的方法就是边看文档边实践，这篇文章也是我在看完babel的文档后实践的。附一下babel的文档： https://github.com/jamiebuilds/babel-handbook/blob/master/translations/zh-Hans/plugin-handbook.md 2.2 使用astexplorer辅助编写ast脚本这其实是一个网站，是一个左右布局的页面，左边写入js代码，右边会给它的抽象语法树。在编写解混淆代码时可以使用它进行辅助。 2.3 分析混淆后的代码通过vscode打开后，将所有代码快折叠，可以看到开头有一个AFiup函数，然后在它内部又定义了几个函数，在最后有一个很长的自执行函数。 浏览一下最后的自执行函数，不难发现里面有很多调用AFiup.$_CP函数的代码块，放到浏览器执行一下发现这个函数在传入一个数字后返回一个字符串。好，那第一步就是还原对AFiup.$_CP函数的调用。 2.4 还原AFiup.$_CP函数调用这里需要说一下，为什么可以对AFiup.$_CP的调用进行还原，可以看下它的特征： 代码中对其进行调用的参数都是字面量，像659、89这种，不涉及到其他的变量或者是函数之类的。 $_CP函数在传入参数不变的情况下，无论调用次数或执行环境如何变化，函数始终返回相同的计算结果，也就是说它是一个纯函数或者叫确定性函数。当然这需要在浏览器和nodejs中进行验证，才能得到的结论，这里就略过了。既然它是一个纯函数，我们就可以直接把它在反混淆脚本中定义好，然后在使用babel遍历语法树，在调用的地方直接用结果将调用节点替换掉。比如说下面代码中的$_BIJHw(89) ``$_BIJHw(34) 、 1234567891011var $_BIJHw = AFiup.$_CP;var $_BIJGv = [&quot;$_BJAAQ&quot;].concat($_BIJHw);var $_BIJID = $_BIJGv[1];$_BIJGv.shift();var $_BIJJe = $_BIJGv[0];var _;var e = Object[$_BIJHw(89)];var c = e[$_BIJHw(34)];var t; 思路如下： 1. 首先定位到类似var $_BIJHw = AFiup.$_CP;语句 1. 查找$_BIJHw变量所有的引用位置，找到其中类似$_BIJHw(xxx)的位置，其中xxx意思是纯数字 1. 拿到xxx值，本地调用后替换节点.完整的visitor代码如下： 1234567891011121314151617181920212223242526272829303132333435const replaceCallAFiup$_CP = { VariableDeclaration(path) { // 定位到类似的语句var _ccc = AFiup.$_CP; if ( types.isVariableDeclaration(path.node) &amp;&amp; path.node.declarations.length === 1 &amp;&amp; types.isVariableDeclarator(path.node.declarations[0]) &amp;&amp; types.isIdentifier(path.node.declarations[0].id) &amp;&amp; types.isMemberExpression(path.node.declarations[0].init) &amp;&amp; types.isIdentifier(path.node.declarations[0].init.object) &amp;&amp; types.isIdentifier(path.node.declarations[0].init.property) &amp;&amp; path.node.declarations[0].init.property.name === &quot;$_CP&quot; ) { var varName = path.node.declarations[0].id.name; // 查找varName变量的所有引用 let binding = path.scope.getBinding(varName); binding.referencePaths.forEach(refPath =&gt; { if ( types.isCallExpression(refPath.parent) &amp;&amp; refPath.parent.arguments.length === 1 &amp;&amp; types.isNumericLiteral(refPath.parent.arguments[0]) ) { let callArgumentInt = refPath.parent.arguments[0].value; console.log(`${refPath.parentPath.toString()} --&gt; ${AFiup.$_CP(callArgumentInt)}`) refPath.parentPath.replaceWith(types.valueToNode(AFiup.$_CP(callArgumentInt))) } }); } // 最后别忘了刷新scope path.scope.crawl(); },}; 执行以下可以看到替换掉了很多的节点：然后接着分析代码，还有一部分AFiup.$_CP函数的调用，是比较绕的，先是赋值给变量$_CHIHc，接着将其concat到$_CHIGy然后变量$_CHIJT又从$_CHIGy中取出来。最后在通过调用$_CHIIJ的方式来调用AFiup.$_CP函数。思考一下，这里如果将var $_CHIJT = $_CHIGy[0];替换成var $_CHIJT = AFiup.$__CP就可以直接复用上一步写的visitor了。那思路就有了： 依次匹配语句1、2、3、4、5 将语句5进行替换，格式:var xxx = AFiup.$_CP 123456var $_CHIHc = AFiup.$_CP; // 语句1var $_CHIGy = [&quot;$_CHJAQ&quot;].concat($_CHIHc); // 语句2var $_CHIIJ = $_CHIGy[1]; // 语句3$_CHIGy.shift(); // 语句4var $_CHIJT = $_CHIGy[0]; // 语句5e[$_CHIIJ(1220)](); 完整的visitor代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940const replaceCallAFiup$_CP = { VariableDeclaration(path) { if ( types.isVariableDeclaration(path.node) &amp;&amp; path.node.declarations.length === 1 &amp;&amp; types.isVariableDeclarator(path.node.declarations[0]) &amp;&amp; types.isIdentifier(path.node.declarations[0].id) &amp;&amp; types.isMemberExpression(path.node.declarations[0].init) &amp;&amp; types.isIdentifier(path.node.declarations[0].init.object) &amp;&amp; types.isLiteral(path.node.declarations[0].init.property) &amp;&amp; path.node.declarations[0].init.property.value === 1 ) { const memberExpression = types.memberExpression( types.identifier(&quot;AFiup&quot;), // 对象：AFiup types.identifier(&quot;$_CP&quot;), // 属性：$_CP false // 非计算属性（即用点符号访问，而非方括号） ); // 创建变量声明符 $_FCIU = AFiup.$_CP const declarator = types.variableDeclarator( types.identifier( path.node.declarations[0].id.name ), // 变量名 memberExpression // 初始值 ); // 创建完整的变量声明节点 var $_FCIU = AFiup.$_CP const newNode = types.variableDeclaration( &quot;var&quot;, // kind: var [declarator] // declarations 数组 ); const beforeCode = path.toString(); path.replaceWith(newNode); console.log(`${beforeCode} -&gt; ${path.toString()}`); } // 定位到类似的语句var _ccc = AFiup.$_CP; ...}; 2.5 移除无用代码很简单的操作不多说了 1234567891011121314151617181920212223242526272829303132333435363738394041const pluginRemoveUnuseVarDeclaration = { VariableDeclaration(path) { const declarations = path.node.declarations; // 收集所有变量的引用信息 const bindings = new Map(); declarations.forEach((decl) =&gt; { const binding = path.scope.getBinding(decl.id.name); if (binding) { bindings.set(decl.id.name, binding.referencePaths.length); } }); // 过滤出未使用的变量声明 const unusedDeclarations = declarations.filter((decl) =&gt; { return bindings.get(decl.id.name) === 0; }); // 如果所有声明都未使用，删除整个声明语句 if (unusedDeclarations.length === declarations.length) { console.log( `移除整个未使用的变量声明语句，包含 ${declarations .map((d) =&gt; d.id.name) .join(&quot;, &quot;)}` ); path.remove(); } // 否则只删除未使用的声明 else if (unusedDeclarations.length &gt; 0) { console.log( `移除部分未使用的变量: ${unusedDeclarations .map((d) =&gt; d.id.name) .join(&quot;, &quot;)}` ); path.node.declarations = declarations.filter( (decl) =&gt; !unusedDeclarations.includes(decl) ); } path.scope.crawl(); },}; 3. 分析反混淆后的代码替换掉AFiup.$_CP函数调用后代码基本上就可读了，后面就可以用反混淆的文件替换掉浏览器上的进行分析了。","link":"/posts/e474db30.html"},{"title":"某安X-App-Token参数逆向|基于unidbg模拟调用生成","text":"记录使用unidbg还原so层加密函数的一次尝试 目标使用unidbg对APP的接口签名字段X-App-Token进行生成。 目标APP: 6YW35a6JVjEzLjEuMQ== 1. 抓包分析这一步很简单，使用Postern配置charles就可以完成抓包，找到获取动态详情的接口https://api2.coolapk.com/v6/feed/detail?id=44874711其中id为发文的id，这是一个接受post请求的接口，其中在请求headers中有两个醒目的字段X-App-Version和X-App-Device，经过测试两个字段缺一不可。 缺少字段的其中任一字段或进行篡改会提示请求未验证 12345{ &quot;status&quot;: 1004, &quot;error&quot;: null, &quot;message&quot;: &quot;请求未验证&quot;} 2. 反编译APP这一步我们使用jadx进行反编译，需要注意这个app使用了360的加固，需要进行脱壳处理，我这使用https://github.com/hluwa/frida-dexdump进行脱壳。 安装frida-dexdump可以直接使用pip进行安装pip install frida-dexdump，要注意版本需要和你使用的frida-server版本对应。 这里提供我的版本信息供参考 版本 备注 python 3.7 frida-server 16.0.0 frida-dexdump 2.0.1 frida-tools 12.0.1 之后在手机端打开某安APP，电脑端命令行中执行frida-dexdump -FU进行脱壳，-FU的含义是指定前台应用。 完成之后会在当前目录下的{APP_NAME}目录中写入脱出来的dex文件。不过这时候脱出来的dex文件不能直接拿到jadx中进行反编译还需要使用https://github.com/luoyesiqiu/DexRepair进行头部修复。这里我写了一个小脚本来调用 123456789101112131415161718192021222324import clickimport pathlibimport osfrom loguru import loggerfrom tqdm import tqdm@click.command()@click.option(&quot;--path&quot;, default=&quot;./&quot;, help=&quot;dex文件目录所有路径&quot;)def main(path): logger.info(&quot;&quot;&quot; 需要在当前目录下放置文件&quot;DexRepair.jar&quot;\\n 可在: &quot;https://github.com/luoyesiqiu/DexRepair&quot; 下载 &quot;&quot;&quot;) path = pathlib.Path(path) for file_path in tqdm(path.rglob(&quot;*.dex&quot;)): logger.info(&quot;start {}&quot;.format(file_path)) os.system(&quot;java -jar DexRepair.jar {}&quot;.format(file_path)) logger.info(&quot;completed&quot;)if __name__ == &quot;__main__&quot;: main() 将修复后的dex文件拖入到jadx中进行分析。 3. 分析字段生成逻辑在jadx中全局搜索X-App-Token(这一步操作可能会很卡)。结果只有一条，不用纠结，就是它 点进去可以看到appToken由getAS函数生成 接着使用objection对这个函数进行hook。 步骤： adb 连接手机，开启frida-server 重开一个 shell窗口指定命令objection -g &quot;com.coolapk.market&quot; explore 进入objection后在交互窗口指定命令android hooking watch class_method 'com.coolapk.market.util.AuthUtils.getAS' --dump-args --dump-return 没有问题的话会输出提示: 📢这里在使用Objection hook前有个小细节，需要先将app卸载，之后断网环境下重装app，hook成功后再打开网络。如果不这么做的话会提示你Not found class。猜测是由于360加固的问题，断网是为了防止360加固通过网络更新。 打开网络后，我试着手机上滑动一下app，在objection的窗口那，会显示一些信息。可以看到已经打印出了getAS函数的入参和返回值。 对比一下请求中的headers不难发现第二个参数就是X-App-Device返回值就是X-App-Token。接着我们进一步分析m44554函数 有两个关键字: base64 reverse 合理猜测X-App-Device是经过反转后的base64编码，接着我们X-App-Device反转回去并尝试base64解码. 最终拿到x-app-device的明文值DUzuPzp3eMbe8yfL6IeDaAfOOI0Q7ts0KSbc; ; ; ; Google; google; Pixel 3; SP1A.210812.016.C2; null，这个字符串分了好几个部分，测试后发现第一部分的字符串可以进行篡改不应该最终结果，这里就不再深入了。 总结一下，X-App-Device参数是由一些信息经过base64编码后又反转得到的；X-App-Token是由getAS函数进行加密得到的。 4. 使用unidbg进行模拟调用生成X-App-Token在jadx中查看一下getAS函数，可以发现这是一个由jni进行注册的函数，对应so文件的名称是native-lib。接下来编写unidbg脚本进行模拟调用。 这里不多介绍unidbg的使用，调用脚本源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.kuan;import com.github.unidbg.AndroidEmulator;import com.github.unidbg.arm.backend.Unicorn2Factory;import com.github.unidbg.linux.android.AndroidEmulatorBuilder;import com.github.unidbg.linux.android.AndroidResolver;import com.github.unidbg.linux.android.dvm.*;import com.github.unidbg.memory.Memory;import com.lvzhou.xvideo;import java.io.File;public class xAppToken extends AbstractJni{ private final AndroidEmulator emulator; private final DvmClass NativeApi; private final VM vm; public xAppToken(){ // 初始化一个模拟器 // for64Bit代表使用arm64，一般arm32更常见 // setProcessName 设置进程名，推荐如实设置。如果不设置样本在调用getprogname时会返回'unidbg'可能会触发反爬 emulator = AndroidEmulatorBuilder .for32Bit()// .addBackendFactory(new Unicorn2Factory(true)) // 设置指令执行引擎. .setProcessName(&quot;com.coolapk.market &quot;) .build(); Memory memory = emulator.getMemory(); memory.setLibraryResolver(new AndroidResolver(23)); vm = emulator.createDalvikVM(new File(&quot;unidbg/unidbg-android/src/test/resources/kuan/kuan.apk&quot;)); vm.setJni(this); vm.setVerbose(true); DalvikModule dm = vm.loadLibrary(&quot;native-lib&quot;, true); // 声明一个DvmClass类// NativeApi = vm.resolveClass(&quot;com/weibo/xvideo/NativeApi&quot;); NativeApi = vm.resolveClass(&quot;com/coolapk/market/util/AuthUtils&quot;); dm.callJNI_OnLoad(emulator); } public static void main(String[] args) { xAppToken t = new xAppToken();// String result = xv.calls();// System.out.println(result); t.callGetAS(); } public String callGetAS(){// emulator.traceCode(); DvmObject&lt;?&gt; context = vm.resolveClass(&quot;android/content/Context&quot;).newObject(null); String xAppDevice = &quot;RFU5Q1Bsd2FGcTVKS0gxYU1KSHI4bkZaNWxhd1BYbzFnazk1OyA7IDsgOTA6RjA6NTI6OTQ6NUQ6QzU7IG1laXp1OyBtZWl6dTsgMTZzIFBybzsgRmx5bWUgOC4xLjguMEE7IDg0YzFkN2U2ODcwMjBjY2U4ODIxM2ExZmFjMjE2OTU0&quot;; String res = NativeApi.newObject(null).callJniMethodObject(emulator, &quot;getAS(Landroid/content/Context;Ljava/lang/String;)Ljava/lang/String;&quot;, context, xAppDevice).getValue().toString(); System.out.println(res); return res; }} 执行结果如图: 这里用的知识只是unidbg的基本使用，并没有涉及到补环境，还是非常简单的。 5. 总结unidbg模拟调用相对于frida rpc来说成本相当低，只需要一台有java环境的服务器即可。","link":"/posts/507c1bbe.html"},{"title":"梳理python的可迭代对象、迭代器、生成器","text":"在Python中这几个概念经常用到，但是也很容易被混淆。写这篇笔记，做个简单的总结。 说在前面的一些结论： 迭代器一定是一个可迭代对象，可迭代对象却不一定是迭代器。 可迭代对象可以通过Iter()函数转化为迭代器。 可迭代对象一定可以使用for循环迭代，能使用for循环迭代的不一定是可迭代对象 可迭代对象(Iterable)先来看可迭代对象, 简单来说在python中的一个对象如果实现了__iter__()方法(这个方法的具体作用放到后面说)，那个这个对象就是可迭代对象(Iterable)。例如常见的可迭代对象有： 集合或序列(如list、tuple、set、dict、str) 文件对象 实现了__iter__()方法的对象 来验证一下上述提到的是否真的是可迭代对象，并且是否都实现了__iter__()方法 使用python内置的isinstance可以用来验证对象是否是可迭代对象。使用内置hasattr函数来验证对象是否实现了__iter__方法。 1234567891011121314151617181920212223242526272829303132from collections.abc import Iterable# 验证列表print(hasattr([], &quot;__iter__&quot;)) # print trueprint(isinstance([], Iterable)) # print true# 验证元组print(hasattr((), &quot;__iter__&quot;)) # print trueprint(isinstance((), Iterable)) # print true# 验证集合print(hasattr(set(), &quot;__iter__&quot;)) # print trueprint(isinstance(set(), Iterable)) # print true# 验证字典print(hasattr(dict(), &quot;__iter__&quot;)) # print trueprint(isinstance(dict(), Iterable)) # print true# 验证字符创print(hasattr(&quot;&quot;, &quot;__iter__&quot;)) # print trueprint(isinstance(&quot;&quot;, Iterable)) # print true# 验证文件对象with open(&quot;hello.py&quot;, &quot;w&quot;) as f: print(hasattr(f, &quot;__iter__&quot;)) # print true print(isinstance(f, Iterable)) # print true# 我们自己来实现一个可迭代对象 class IterObj(object): def __iter__(self): # 注意 这只是一个例子，如果这个类的实例化对象拿到for循环里是会报错的，下面再讲为什么 return self# 验证实现了 __iter__的对象obj = IterObj()print(isinstance(obj, Iterable)) # print true 迭代器(Iterator)再来看一下迭代器的概念，简单来说，一个对象实现了 __iter__和__next__方法，那么它就是一个迭代器。例如我们在上一段代码中自己实现的可迭代对象Iterable只要再加上一个__next__方法，那么它的实例化对象就可以称为一个迭代器。 我们可以使用python的内置函数next对迭代器进行调用，只到迭代器抛出异常StopIteration。例如： 1234567891011121314151617181920212223class IterObj(object): def __init__(self): self.arr = [1, 2, 3] self.length = len(self.arr) self.i = 0 def __iter__(self): return iter(self.arr) def __next__(self): while self.i &lt; self.length: self.i += 1 return self.arr[self.i - 1] else: self.i = 0 raise StopIteration()obj = IterObj()print(next(obj)) # print 1print(next(obj)) # print 2print(next(obj)) # pinrt 3print(next(obj)) # 抛出异常 在上面的代码中我们在构造函数中定义了一个列表，在__iter__方法中我们返回了iter(self.arr)。那么这个iter()是个什么东西呢？ __iter__和iter()前面已经说过了，如果一个对象实现了__iter__方法，那么这个对象就是可迭代对象。那这个方法具体做了些什么呢？这个方法，返回的是一个迭代器(Iterator). iter()这个内置函数返回的也是一个迭代器。它的工作过程是，先访问对象的__iter__方法，如果不存在则访问__getitem__方法。这就是结论2 的由来。 那这个Iter()是给谁用的呢？只有我们显式的调用吗？我们回顾一下对列表的一个常见的操作： 使用for循环对其进行迭代，在这个过程当中，for循环的工作过程是这样的： 先判断对象是否是为可迭代对象，或者对象是否实现了__getitem__()方法(先留个坑待会再填)。如果是的话，则对对象调用Iter方法，否则抛出异常。所以在我们自己实现的一个可迭代对象中，我们必须要保证__iter__这个方法返回的是一个迭代器。 不断的调用迭代器里的__next__方法，返回迭代器里的一个值。 迭代的最后，迭代器里没有元素了，就会抛出一个异常，for循环结束。 对于其他的可迭代对象，过程也是一样的。弄明白for循环的工作原理后，就可以来动手实现一个可迭代对象了。 通常我们可以使用已有的可迭代对象来辅助我们实现自定义的可迭代对象， 也可以通过同时实现__next__和__iter__来自定义一个迭代器(Iterator)。 123456789101112131415161718192021222324252627282930313233# 借助已知的可迭代对象来自定义可迭代对象class MyList(): def __init__(self): self.my_list = [1, 2, 3] def __iter__(self): return iter(self.my_list)# 同时实现 `__next__`和`__iter__`来自定义一个迭代器(`Iterator`)。class MyRange(): def __init__(self, num): self.i = 0 self.num = num def __iter__(self): return self def __next__(self): if self.i &lt; self.num: res = self.i self.i += 1 return res else: raise StopIterationmy_iterable = MyRange(10)for i in my_iterable: print(i) # 打印0-9my_iterable = MyList()for i in my_iterable: print(i) # 打印 1-3 生成器(generator`)生成器本质上还是一个迭代器。它也可以用在迭代的操作中，它和迭代器的区别是，它是一遍迭代一遍计算。在对生成器调用next()函数时，生成器才会计算需要返回的值，在这之前需要返回的值是不存在的，而迭代器在调用next()函数前，返回值已经存在了。 定义生成器有两种方式: 列表生成器 使用yield定义生成器函数 使用列表生成器1234567gen = (i*i for i in range(3)) # 需要把列表生成器的[]改为()print(next(gen)) # print 0print(next(gen)) # print 1print(next(gen)) # pirnt 4# 也可以使用for循环for i in gen: print(i) 使用yield关键字举个栗子🌰，斐波那契数列： 123456789def fib(n): prev, curr = 0, 1 while n &gt; 0: n -= 1 yield curr prev, curr = curr, curr + prevprint([i for i in fib(10)])#[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 另外python使用生成器的特点实现了协程，它相对于线程处理高并发场景具有更大的优势。","link":"/posts/951a8626.html"},{"title":"踩坑-python创建二维数组","text":"在做一道动态规划的题需要用到一个二维数组。看到二维数组很快就能想到可以用嵌套的list来实现，但是这里面是有个坑的。 例如创建一个4X4的二维数组，全部值置为1. 错误的代码：1a = [[1]*4]*4 # 类似于 ‘+’*100 输出结果如下，看起来一点问题都没有！ 1a = [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]] 接着对a中某个单独的值进行修改。 123print(&quot;a = &quot;, a) # a原来的值a[2][2] = 2print('a = ', a) # 修改后a的值 输出结果如下： 12a = [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]] # 修改a的值a = [[1, 1, 2, 1], [1, 1, 2, 1], [1, 1, 2, 1], [1, 1, 2, 1]] # 修改后a的值 对比一下，a[2][2] = 2这个操作它修改了a[2]而不是a[2][2].？？？？？惊喜！！！ 正确的方法 列表生成式 1a = [[1 for i in range(4)] for j in range(4)] 列表生成式真的太有用了！！！ 使用numpy模块创建 12import numpy as np # 需要安装numpya = np.ones(16).reshape(4,4) 解释list * n 相当于是 n个list的浅拷贝的连接。下面代码打印了a中每个子列表的id值。 12for i in range(4): print(id(a[i])) 结果如下： 12344460725424446072542444607254244460725424 可以看到a中每个子列表的id值都是相同的，即它们引用的是同一个列表。当对其中的某个子列表进行修改时，其它子列表自然也改变了。","link":"/posts/d9568da.html"}],"tags":[{"name":"AES","slug":"AES","link":"/tags/AES/"},{"name":"MD5","slug":"MD5","link":"/tags/MD5/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"picgo","slug":"picgo","link":"/tags/picgo/"},{"name":"图床","slug":"图床","link":"/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"mac使用技巧","slug":"mac使用技巧","link":"/tags/mac%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"极验","slug":"极验","link":"/tags/%E6%9E%81%E9%AA%8C/"},{"name":"ast","slug":"ast","link":"/tags/ast/"},{"name":"APP逆向","slug":"APP逆向","link":"/tags/APP%E9%80%86%E5%90%91/"},{"name":"unidbg","slug":"unidbg","link":"/tags/unidbg/"}],"categories":[{"name":"JS逆向","slug":"JS逆向","link":"/categories/JS%E9%80%86%E5%90%91/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"归档","slug":"归档","link":"/categories/%E5%BD%92%E6%A1%A3/"},{"name":"爬虫","slug":"爬虫","link":"/categories/%E7%88%AC%E8%99%AB/"}],"pages":[]}