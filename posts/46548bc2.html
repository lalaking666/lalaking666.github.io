<!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>scrapy爬虫苏宁易购图书信息 - Lalaking sharing</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lalaking sharing"><meta name="msapplication-TileImage" content="favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lalaking sharing"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="在刚刚入门scrapy后尝试写了一个小的爬虫，目标是爬取苏宁易购下的所有的图书信息."><meta property="og:type" content="blog"><meta property="og:title" content="scrapy爬虫苏宁易购图书信息"><meta property="og:url" content="https://www.huihuidehui.com/posts/46548bc2.html"><meta property="og:site_name" content="Lalaking sharing"><meta property="og:description" content="在刚刚入门scrapy后尝试写了一个小的爬虫，目标是爬取苏宁易购下的所有的图书信息."><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.huihuidehui.com/img/5.jpg"><meta property="article:published_time" content="2019-10-01T05:44:50.000Z"><meta property="article:modified_time" content="2019-10-01T05:44:50.000Z"><meta property="article:author" content="lalaking"><meta property="article:tag" content="Python"><meta property="article:tag" content="爬虫"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://www.huihuidehui.com/img/5.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.huihuidehui.com/posts/46548bc2.html"},"headline":"scrapy爬虫苏宁易购图书信息","image":["https://www.huihuidehui.com/img/5.jpg"],"datePublished":"2019-10-01T05:44:50.000Z","dateModified":"2019-10-01T05:44:50.000Z","author":{"@type":"Person","name":"lalaking"},"publisher":{"@type":"Organization","name":"Lalaking sharing","logo":{"@type":"ImageObject","url":"https://www.huihuidehui.com/posts/favicon.svg"}},"description":"在刚刚入门scrapy后尝试写了一个小的爬虫，目标是爬取苏宁易购下的所有的图书信息."}</script><link rel="canonical" href="https://www.huihuidehui.com/posts/46548bc2.html"><link rel="icon" href="/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?2dfd31fccde8a41229f6af7e2d093711";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-152746704-2" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-152746704-2")</script><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><script>!function(){function e(){if(!location.hash)return;const e="#"+CSS.escape(location.hash.substring(1)),t=document.querySelector(`.tabs a[href="${e}"]`);if(!t)return;const n=t.parentElement.parentElement;Array.from(n.children).forEach(e=>e.classList.remove("is-active")),Array.from(n.querySelectorAll("a")).map(e=>document.getElementById(e.getAttribute("href").substring(1))).forEach(e=>e.classList.add("is-hidden")),t&&t.parentElement.classList.add("is-active");const r=document.querySelector(e);r&&r.classList.remove("is-hidden")}e(),window.addEventListener("hashchange",e,!1)}()</script><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/favicon.svg" alt="Lalaking sharing" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives/index.html">归档</a><a class="navbar-item" href="/categories/index.html">分类</a><a class="navbar-item" href="/tags/index.html">标签</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener external nofollow noreferrer" title="Download on GitHub" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/5.jpg" alt="scrapy爬虫苏宁易购图书信息"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2019-10-01T05:44:50.000Z" title="2019/10/1 13:44:50">2019-10-01</time>发表</span><span class="level-item"><time datetime="2019-10-01T05:44:50.000Z" title="2019/10/1 13:44:50">2019-10-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span><span class="level-item">13 分钟读完 (大约1962个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">scrapy爬虫苏宁易购图书信息</h1><div class="content"><p>在刚刚入门scrapy后尝试写了一个小的爬虫，目标是爬取苏宁易购下的所有的图书信息.</p><span id="more"></span><p>就是下面的每个大分类下的所有小分类的所有所售图书信息。我所抓取的数据有每个图书所在的大分类、小分类、作者、出版社、价格（这个数据的获取稍微麻烦一点）。</p><img src="https://img-blog.csdnimg.cn/20190902193357314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbl9odWk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%"><h1 id="第一步：-分析网页"><a href="#第一步：-分析网页" class="headerlink" title="第一步： 分析网页"></a>第一步： 分析网页</h1><p>首先需要找到我们要爬取的数据的位置。以及分析怎么在python中提取我们需要的数据。</p><h3 id="1-1-找到要爬取的数据位置"><a href="#1-1-找到要爬取的数据位置" class="headerlink" title="1.1 找到要爬取的数据位置"></a>1.1 找到要爬取的数据位置</h3><h2 id="分类信息"><a href="#分类信息" class="headerlink" title="分类信息"></a>分类信息</h2><ul><li><p>打开谷歌浏览器进入<a target="_blank" rel="noopener external nofollow noreferrer" href="https://book.suning.com/按F12进入控制台，进入network选项后，刷新一下。如下图所示，在第一个请求获取的响应中已经是包含了所有分类的url地址了。所以可以把此url当作初始url。">https://book.suning.com/按F12进入控制台，进入network选项后，刷新一下。如下图所示，在第一个请求获取的响应中已经是包含了所有分类的url地址了。所以可以把此url当作初始url。</a><img src="https://img-blog.csdnimg.cn/20190902194349715.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbl9odWk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:33%"></p><h2 id="图书信息"><a href="#图书信息" class="headerlink" title="图书信息"></a>图书信息</h2></li><li><p>接下寻找图书的详细信息所在的位置。随便打开一个图书的链接。进入控制台查看第一个请求所获得的响应。如下图所示可以看到我们需要获取的信息除了价格外全都有了。<img src="https://img-blog.csdnimg.cn/20190902194825261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbl9odWk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:33%"></p><h2 id="价格信息"><a href="#价格信息" class="headerlink" title="价格信息"></a>价格信息</h2></li><li><p>现在只剩下一个价格信息了，在图书的详情页面打开控制台找到nsp开头的那个请求，价格就在这个请求里面。如下图。查看这个请求的url地址可以发现有两个参数是不确定的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pas.suning.com/nspcsale_0_000000010227414878_000000010227414878_0070183717_120_536_5360101_502282_1000235_9232_11809_Z001___R9011546_1.0___.html">https://pas.suning.com/nspcsale_0_000000010227414878_000000010227414878_0070183717_120_536_5360101_502282_1000235_9232_11809_Z001___R9011546_1.0___.html</a><br>其中的10227414878，701837171是不确定的。这两个参数的是在图书的详情页面的链接中是有的。直接正则提取就可以（比如这个<a target="_blank" rel="noopener external nofollow noreferrer" href="https://product.suning.com/0070183717/10227414878.html?srcpoint=pindao_book_27582454_prod01）">https://product.suning.com/0070183717/10227414878.html?srcpoint=pindao_book_27582454_prod01）</a><br>后面的R9011546是可以去掉的。通过多观察几个图书的请求价格的url链接可以发现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">120_536_5360101_502282_1000235_9232_11809_Z001_</span><br></pre></td></tr></table></figure><p>是固定的。所以最终请求价格的url地址<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pas.suning.com/nspcsale_0_{}_{}_{}_120_536_5360101_502282_1000235_9232_11809_Z001_1.0___.html&quot;。">https://pas.suning.com/nspcsale_0_{}_{}_{}_120_536_5360101_502282_1000235_9232_11809_Z001_1.0___.html&quot;。</a></p><p>{}部分在代码中会用format函数格式化。</p><img src="https://img-blog.csdnimg.cn/20190902195409295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbl9odWk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%"> # 1.2 分析如何获取数据 正则 or xpath? 找到了需要的数据所在的位置后，下一步就是从网页中提取数据。 ## 提取分类的url</li><li><p>可以在谷歌浏览器中使用xpath helper尝试使用xpath定位到每个小分类的位置。并获取分类的url。用代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取每个大分类下的所有小分类并遍历获取每个小分类对应的url地址</span></span><br><span class="line">categories_big = response.xpath(<span class="string">&quot;//div[@class=&#x27;menu-list&#x27;]/div&quot;</span>) <span class="comment"># 这里的reponse是scrapy默认的parse函数里的参数</span></span><br><span class="line">        <span class="keyword">for</span> category_big <span class="keyword">in</span> categories_big:</span><br><span class="line">            detail_categories = category_big.xpath(<span class="string">&quot;./dl/dd/a&quot;</span>)</span><br><span class="line">            <span class="keyword">for</span> detail_category <span class="keyword">in</span> detail_categories:</span><br><span class="line">            	<span class="comment"># 得到小分类的url地址</span></span><br><span class="line">                category_detail_url = detail_category.xpath(<span class="string">&quot;./@href&quot;</span>).extract_first()</span><br></pre></td></tr></table></figure><h2 id="获取图书信息"><a href="#获取图书信息" class="headerlink" title="获取图书信息"></a>获取图书信息</h2></li><li><p>获取图书信息这一步可以使用正则表达式来完成简单粗暴！！！。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先用xpath的获取1.1图书信息图中的meta标签的值。</span></span><br><span class="line">      res = response.xpath(<span class="string">&quot;//meta[@property=&#x27;og:description&#x27;]/@content&quot;</span>).extract_first()</span><br><span class="line">      <span class="comment"># 下面的代码是直接正则拿到数据。</span></span><br><span class="line">      item = &#123;&#125;</span><br><span class="line">      item[<span class="string">&#x27;book_name&#x27;</span>] = re.findall(<span class="string">r&quot;《(.*)》，作者&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;《(.*)》，作者&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      <span class="comment"># item[&#x27;author&#x27;] = re.findall(r&quot;作者：(.*)著&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)著&quot;, res)) else &#x27;&#x27;</span></span><br><span class="line">      item[<span class="string">&#x27;author&#x27;</span>] = re.findall(<span class="string">r&quot;作者：(.*)，.*出版&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;作者：(.*)，.*出版&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      item[<span class="string">&#x27;publisher&#x27;</span>] = re.findall(<span class="string">r&quot;作者：.*，(.*)出版&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;作者：.*，(.*)出版&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      item[<span class="string">&#x27;publisher&#x27;</span>] = item[<span class="string">&#x27;publisher&#x27;</span>] + <span class="string">&#x27;出版社&#x27;</span></span><br><span class="line">      item[<span class="string">&#x27;big_category&#x27;</span>] = re.findall(<span class="string">r&quot;品类：(.*)&gt;&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;品类：(.*)&gt;&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      item[<span class="string">&#x27;detail_category&#x27;</span>] = re.findall(<span class="string">r&quot;&gt;(.*)，以及&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;&gt;(.*)，以及&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="价格"><a href="#价格" class="headerlink" title="价格"></a>价格</h2></li><li><p>前面已经得到了获取价格的url地址直接请求就可以，返回的数据是直接正则拿到价格。这个简单就不详细说了。主要是这个数据还需要另外请求一个url就比较麻烦。。。</p><h2 id="第二步：-开始编写代码"><a href="#第二步：-开始编写代码" class="headerlink" title="第二步： 开始编写代码"></a>第二步： 开始编写代码</h2><h3 id="2-1-新建scrapy项目"><a href="#2-1-新建scrapy项目" class="headerlink" title="2.1 新建scrapy项目"></a>2.1 新建scrapy项目</h3></li><li><p>这里我假设你已经安装好了python3并且装了scrapy模块。如果没有，百度一下？教程很多！</p></li><li><p>选择一个合适的目录打开终端</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject suning</span><br><span class="line">cd suning</span><br><span class="line">scrapy genspider book suning.com</span><br></pre></td></tr></table></figure><h3 id="2-2-项目结构"><a href="#2-2-项目结构" class="headerlink" title="2.2 项目结构"></a>2.2 项目结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">(13-爬虫) kanhui@kanhui-Mac ~/D/2/2/1/suning&gt; tree</span><br><span class="line">.</span><br><span class="line">├── scrapy.cfg</span><br><span class="line">└── suning</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── __pycache__</span><br><span class="line">    │   ├── __init__.cpython-37.pyc</span><br><span class="line">    │   ├── items.cpython-37.pyc</span><br><span class="line">    │   ├── pipelines.cpython-37.pyc</span><br><span class="line">    │   └── settings.cpython-37.pyc</span><br><span class="line">    ├── items.py</span><br><span class="line">    ├── middlewares.py</span><br><span class="line">    ├── pipelines.py</span><br><span class="line">    ├── settings.py</span><br><span class="line">    ├── setup.py</span><br><span class="line">    └── spiders</span><br><span class="line">        ├── __init__.py</span><br><span class="line">        ├── __pycache__</span><br><span class="line">        │   ├── __init__.cpython-37.pyc</span><br><span class="line">        │   └── book.cpython-37.pyc</span><br><span class="line">        └── book.py</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-3详细代码"><a href="#2-3详细代码" class="headerlink" title="2.3详细代码"></a>2.3详细代码</h3><h4 id="book-py"><a href="#book-py" class="headerlink" title="book.py"></a>book.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># ************************************************************************</span></span><br><span class="line"><span class="comment"># *</span></span><br><span class="line"><span class="comment"># * @file:book.py</span></span><br><span class="line"><span class="comment"># * @author:kanhui</span></span><br><span class="line"><span class="comment"># * date:2019-09-01 14:01:05</span></span><br><span class="line"><span class="comment"># * @version vim 8</span></span><br><span class="line"><span class="comment"># *</span></span><br><span class="line"><span class="comment"># ************************************************************************</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> suning.items <span class="keyword">import</span> SuningItem</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BookSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;book&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;suning.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://book.suning.com/?safp=d488778a.46602.crumbs.2&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;获取图书分类地址，进行分组&#x27;&#x27;&#x27;</span></span><br><span class="line">        categories_big = response.xpath(<span class="string">&quot;//div[@class=&#x27;menu-list&#x27;]/div&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> category_big <span class="keyword">in</span> categories_big:</span><br><span class="line"></span><br><span class="line">            detail_categories = category_big.xpath(<span class="string">&quot;./dl/dd/a&quot;</span>)</span><br><span class="line">            <span class="keyword">for</span> detail_category <span class="keyword">in</span> detail_categories:</span><br><span class="line">                category_detail_url = detail_category.xpath(<span class="string">&quot;./@href&quot;</span>).extract_first()</span><br><span class="line">                <span class="comment"># yield一个request对象</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;爬取分类&quot;</span>)</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                    category_detail_url,</span><br><span class="line">                    callback=<span class="variable language_">self</span>.parse2</span><br><span class="line">                )</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;结束一个分类的爬去&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse2</span>(<span class="params">self, response</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取当前页面的每个图书的信息</span></span><br><span class="line">        results = response.xpath(<span class="string">&quot;//div[@class=&#x27;res-info&#x27;]&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> results:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            book_detail_url = res.xpath(<span class="string">&quot;./p[@class=&#x27;sell-point&#x27;]/a/@href&quot;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            ids = re.findall(<span class="string">r&quot;com/(.*)/(.*).html&quot;</span>, book_detail_url)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(ids):</span><br><span class="line">                item[<span class="string">&#x27;shop_id&#x27;</span>], item[<span class="string">&#x27;prdid&#x27;</span>] = ids[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                item[<span class="string">&#x27;shop_id&#x27;</span>] = item[<span class="string">&#x27;prdid&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># yield一个request对象</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                <span class="string">&#x27;https:&#x27;</span> + book_detail_url,</span><br><span class="line">                callback=<span class="variable language_">self</span>.parse3,</span><br><span class="line">                meta=item</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.base_url = <span class="string">&quot;https://list.suning.com/&#123;&#125;-&#123;&#125;.html&quot;</span></span><br><span class="line">            <span class="comment"># cur_url = response.xpath(&quot;//a[@class=&#x27;&#x27;]/@href&quot;).extract_first()</span></span><br><span class="line">            cur_url = response.url</span><br><span class="line">            <span class="comment"># 当前的页码数恰好和下一页的url地址中的也码数相同。</span></span><br><span class="line"></span><br><span class="line">            prefix, cur_page = re.findall(<span class="string">r&quot;com/(.*)-(.*).html&quot;</span>, cur_url)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">int</span>(cur_page) &lt; <span class="number">100</span>:</span><br><span class="line">                next_url = <span class="variable language_">self</span>.base_url.<span class="built_in">format</span>(prefix, <span class="built_in">str</span>(<span class="built_in">int</span>(cur_page) + <span class="number">1</span>))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;爬取&#123;&#125;页。\n&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(cur_page)))</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                    next_url,</span><br><span class="line">                    callback=<span class="variable language_">self</span>.parse2</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse3</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;书籍详情页面&#x27;&#x27;&#x27;</span></span><br><span class="line">        res = response.xpath(<span class="string">&quot;//meta[@property=&#x27;og:description&#x27;]/@content&quot;</span>).extract_first()</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[<span class="string">&#x27;book_name&#x27;</span>] = re.findall(<span class="string">r&quot;《(.*)》，作者&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;《(.*)》，作者&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># item[&#x27;author&#x27;] = re.findall(r&quot;作者：(.*)著&quot;, res)[0] if len(re.findall(r&quot;作者：(.*)著&quot;, res)) else &#x27;&#x27;</span></span><br><span class="line">        item[<span class="string">&#x27;author&#x27;</span>] = re.findall(<span class="string">r&quot;作者：(.*)，.*出版&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;作者：(.*)，.*出版&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        item[<span class="string">&#x27;publisher&#x27;</span>] = re.findall(<span class="string">r&quot;作者：.*，(.*)出版&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;作者：.*，(.*)出版&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        item[<span class="string">&#x27;publisher&#x27;</span>] = item[<span class="string">&#x27;publisher&#x27;</span>] + <span class="string">&#x27;出版社&#x27;</span></span><br><span class="line">        item[<span class="string">&#x27;big_category&#x27;</span>] = re.findall(<span class="string">r&quot;品类：(.*)&gt;&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;品类：(.*)&gt;&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        item[<span class="string">&#x27;detail_category&#x27;</span>] = re.findall(<span class="string">r&quot;&gt;(.*)，以及&quot;</span>, res)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">r&quot;&gt;(.*)，以及&quot;</span>, res)) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        ids = response.meta</span><br><span class="line">        shop_id = ids.get(<span class="string">&#x27;shop_id&#x27;</span>)</span><br><span class="line">        prdid = ids.get(<span class="string">&#x27;prdid&#x27;</span>)</span><br><span class="line">        prdid = <span class="string">&quot;0&quot;</span> * (<span class="number">18</span> - <span class="built_in">len</span>(prdid)) + prdid</span><br><span class="line">        url = <span class="string">&quot;https://pas.suning.com/nspcsale_0_&#123;&#125;_&#123;&#125;_&#123;&#125;_120_536_5360101_502282_1000235_9232_11809_Z001_1.0___.html&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            prdid, prdid, shop_id)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">            url,</span><br><span class="line">            <span class="variable language_">self</span>.get_price,</span><br><span class="line">            meta=deepcopy(item)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_price</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = response.meta</span><br><span class="line">        price = re.findall(<span class="string">r&#x27;&quot;promotionPrice&quot;:&quot;(.*)&quot;,&quot;bookPrice&quot;&#x27;</span>, response.text)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(</span><br><span class="line">            re.findall(<span class="string">r&#x27;&quot;promotionPrice&quot;:&quot;(.*)&quot;,&quot;bookPrice&quot;&#x27;</span>, response.text)) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        item[<span class="string">&#x27;price&#x27;</span>] = <span class="built_in">float</span>(price)</span><br><span class="line">        book_item = SuningItem()</span><br><span class="line">        book_item[<span class="string">&#x27;book_name&#x27;</span>] = item.get(<span class="string">&#x27;book_name&#x27;</span>)</span><br><span class="line">        book_item[<span class="string">&#x27;author&#x27;</span>] = item.get(<span class="string">&#x27;author&#x27;</span>)</span><br><span class="line">        book_item[<span class="string">&#x27;publisher&#x27;</span>] = item.get(<span class="string">&#x27;publisher&#x27;</span>)</span><br><span class="line">        book_item[<span class="string">&#x27;big_category&#x27;</span>] = item.get(<span class="string">&#x27;big_category&#x27;</span>)</span><br><span class="line">        book_item[<span class="string">&#x27;detail_category&#x27;</span>] = item.get(<span class="string">&#x27;detail_category&#x27;</span>)</span><br><span class="line">        book_item[<span class="string">&#x27;price&#x27;</span>] = item.get(<span class="string">&#x27;price&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> book_item</span><br></pre></td></tr></table></figure><h4 id="item-py"><a href="#item-py" class="headerlink" title="item.py"></a>item.py</h4></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SuningItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    big_category = scrapy.Field()</span><br><span class="line">    detail_category = scrapy.Field()</span><br><span class="line">    book_name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    publisher = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br></pre></td></tr></table></figure><h4 id="piplines-py"><a href="#piplines-py" class="headerlink" title="piplines.py"></a>piplines.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"></span><br><span class="line">client = MongoClient()</span><br><span class="line">collections = client[<span class="string">&#x27;suning&#x27;</span>][<span class="string">&#x27;book1&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SuningPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        collections.insert(<span class="built_in">dict</span>(item))</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h3 id="注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。"><a href="#注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。" class="headerlink" title="注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。"></a>注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。</h3><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><ul><li>程序运行结果如下图，我运行了大概30分钟抓到了30000多条数据。<img src="https://img-blog.csdnimg.cn/20190902203631344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbl9odWk=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:33%"></li></ul></div><div class="article-licensing box"><div class="licensing-title"><p>scrapy爬虫苏宁易购图书信息</p><p><a href="https://www.huihuidehui.com/posts/46548bc2.html">https://www.huihuidehui.com/posts/46548bc2.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>lalaking</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-10-01</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-10-01</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a><a class="link-muted mr-2" rel="tag" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/d9568da.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">踩坑-python创建二维数组</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/4fcb5fd7.html"><span class="level-item">python爬取知乎某个问题下所有图片</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js"></script><script>new Valine({el:"#valine-thread",appId:"X6DQHGReyaU0YRYI51rPUckX-gzGzoHsz",appKey:"i4vQP7YY3tnckaVP7UhjYmMh",avatar:"mm",avatarForce:!1,meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",visitor:!1,highlight:!0,recordIP:!1,enableQQ:!1,requiredFields:[]})</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="辉辉的辉"></figure><p class="title is-size-4 is-block" style="line-height:inherit">辉辉的辉</p><p class="is-size-6 is-block">永远年轻，永远热泪盈眶</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>银河系</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lalaking666" target="_blank" rel="noopener external nofollow noreferrer">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener external nofollow noreferrer" title="Github" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#第一步：-分析网页"><span class="level-left"><span class="level-item">1</span><span class="level-item">第一步： 分析网页</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-找到要爬取的数据位置"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">1.1 找到要爬取的数据位置</span></span></a></li></ul><li><a class="level is-mobile" href="#分类信息"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">分类信息</span></span></a></li><li><a class="level is-mobile" href="#图书信息"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">图书信息</span></span></a></li><li><a class="level is-mobile" href="#价格信息"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">价格信息</span></span></a></li><li><a class="level is-mobile" href="#获取图书信息"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">获取图书信息</span></span></a></li><li><a class="level is-mobile" href="#价格"><span class="level-left"><span class="level-item">1.6</span><span class="level-item">价格</span></span></a></li><li><a class="level is-mobile" href="#第二步：-开始编写代码"><span class="level-left"><span class="level-item">1.7</span><span class="level-item">第二步： 开始编写代码</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-新建scrapy项目"><span class="level-left"><span class="level-item">1.7.1</span><span class="level-item">2.1 新建scrapy项目</span></span></a></li><li><a class="level is-mobile" href="#2-2-项目结构"><span class="level-left"><span class="level-item">1.7.2</span><span class="level-item">2.2 项目结构</span></span></a></li><li><a class="level is-mobile" href="#2-3详细代码"><span class="level-left"><span class="level-item">1.7.3</span><span class="level-item">2.3详细代码</span></span></a></li><li><a class="level is-mobile" href="#注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。"><span class="level-left"><span class="level-item">1.7.4</span><span class="level-item">注意：需要启动mongodb后才可以存储到本地数据库。另外只修改了这三个文件其余文件均未修改。</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#最后"><span class="level-left"><span class="level-item">2</span><span class="level-item">最后</span></span></a></li></ul></div></div><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/JS%E9%80%86%E5%90%91/"><span class="level-start"><span class="level-item">JS逆向</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BD%92%E6%A1%A3/"><span class="level-start"><span class="level-item">归档</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%88%AC%E8%99%AB/"><span class="level-start"><span class="level-item">爬虫</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2025-04-25T01:56:36.000Z">2025-04-25</time></p><p class="title"><a href="/posts/a70d8ab5.html">某壳 wll-kgsa参数算法分析及还原</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2025-03-17T09:02:42.000Z">2025-03-17</time></p><p class="title"><a href="/posts/e474db30.html">极验-3代一键通过模式-反混淆fullpage.9.1.9.cyhomb.js</a></p><p class="categories"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-04-10T06:22:26.000Z">2023-04-10</time></p><p class="title"><a href="/posts/507c1bbe.html">某安X-App-Token参数逆向|基于unidbg模拟调用生成</a></p><p class="categories"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-04-10T02:05:18.000Z">2023-04-10</time></p><p class="title"><a href="/posts/a3dc8a98.html">Picgo搭建Github图床使用Jsdelivr的正确姿势</a></p><p class="categories"><a href="/categories/%E5%BD%92%E6%A1%A3/">归档</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-02-18T08:08:55.000Z">2023-02-18</time></p><p class="title"><a href="/posts/78f06c31.html">JS逆向1: 有道翻译sign参数还原以及api返回数据解密</a></p><p class="categories"><a href="/categories/JS%E9%80%86%E5%90%91/">JS逆向</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">四月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">四月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">八月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/APP%E9%80%86%E5%90%91/"><span class="tag">APP逆向</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MD5/"><span class="tag">MD5</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ast/"><span class="tag">ast</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/flask/"><span class="tag">flask</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mac%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"><span class="tag">mac使用技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/picgo/"><span class="tag">picgo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unidbg/"><span class="tag">unidbg</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E5%BA%8A/"><span class="tag">图床</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%81%E9%AA%8C/"><span class="tag">极验</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag">4</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/favicon.svg" alt="Lalaking sharing" height="28"></a><p class="is-size-7"><span>&copy; 2025 lalaking</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener external nofollow noreferrer">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener external nofollow noreferrer" title="Download on GitHub" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load",()=>{window.cookieconsent.initialise({type:"info",theme:"edgeless",static:!1,position:"bottom-left",content:{message:"此网站使用Cookie来改善您的体验。",dismiss:"知道了！",allow:"允许使用Cookie",deny:"拒绝",link:"了解更多",policy:"Cookie政策",href:"https://www.cookiesandyou.com/"},palette:{popup:{background:"#edeff5",text:"#838391"},button:{background:"#4b81e8"}}})})</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><script type="text/javascript" id="MathJax-script" async>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},chtml:{matchFontHeight:!1}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;" rel="external nofollow noreferrer">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})}))</script></body></html>