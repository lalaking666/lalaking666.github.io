<!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>李宏毅机器学习logistic-regression - Lalaking sharing</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lalaking sharing"><meta name="msapplication-TileImage" content="favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lalaking sharing"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="台大李宏毅机器学习课程第二次作业，logistic regression预测收入。这是我对这个作业的总结，方便日后复习。代码和训练数据已上传至github,戳我"><meta property="og:type" content="blog"><meta property="og:title" content="李宏毅机器学习logistic-regression"><meta property="og:url" content="https://www.huihuidehui.com/posts/2a54212a.html"><meta property="og:site_name" content="Lalaking sharing"><meta property="og:description" content="台大李宏毅机器学习课程第二次作业，logistic regression预测收入。这是我对这个作业的总结，方便日后复习。代码和训练数据已上传至github,戳我"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.huihuidehui.com/img/2.jpg"><meta property="article:published_time" content="2019-11-25T03:34:35.000Z"><meta property="article:modified_time" content="2020-02-11T10:23:23.000Z"><meta property="article:author" content="lalaking"><meta property="article:tag" content="机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://www.huihuidehui.com/img/2.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.huihuidehui.com/posts/2a54212a.html"},"headline":"李宏毅机器学习logistic-regression","image":["https://www.huihuidehui.com/img/2.jpg"],"datePublished":"2019-11-25T03:34:35.000Z","dateModified":"2020-02-11T10:23:23.000Z","author":{"@type":"Person","name":"lalaking"},"publisher":{"@type":"Organization","name":"Lalaking sharing","logo":{"@type":"ImageObject","url":"https://www.huihuidehui.com/posts/favicon.svg"}},"description":"台大李宏毅机器学习课程第二次作业，logistic regression预测收入。这是我对这个作业的总结，方便日后复习。代码和训练数据已上传至github,戳我"}</script><link rel="canonical" href="https://www.huihuidehui.com/posts/2a54212a.html"><link rel="icon" href="/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?2dfd31fccde8a41229f6af7e2d093711";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-152746704-2" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-152746704-2")</script><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><script>!function(){function e(){if(!location.hash)return;const e="#"+CSS.escape(location.hash.substring(1)),t=document.querySelector(`.tabs a[href="${e}"]`);if(!t)return;const n=t.parentElement.parentElement;Array.from(n.children).forEach(e=>e.classList.remove("is-active")),Array.from(n.querySelectorAll("a")).map(e=>document.getElementById(e.getAttribute("href").substring(1))).forEach(e=>e.classList.add("is-hidden")),t&&t.parentElement.classList.add("is-active");const r=document.querySelector(e);r&&r.classList.remove("is-hidden")}e(),window.addEventListener("hashchange",e,!1)}()</script><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/favicon.svg" alt="Lalaking sharing" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives/index.html">归档</a><a class="navbar-item" href="/categories/index.html">分类</a><a class="navbar-item" href="/tags/index.html">标签</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener external nofollow noreferrer" title="Download on GitHub" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/2.jpg" alt="李宏毅机器学习logistic-regression"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2019-11-25T03:34:35.000Z" title="2019/11/25 11:34:35">2019-11-25</time>发表</span><span class="level-item"><time datetime="2020-02-11T10:23:23.000Z" title="2020/2/11 18:23:23">2020-02-11</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a></span><span class="level-item">21 分钟读完 (大约3173个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">李宏毅机器学习logistic-regression</h1><div class="content"><p>台大李宏毅机器学习课程第二次作业，logistic regression预测收入。这是我对这个作业的总结，方便日后复习。代码和训练数据已上传至github,<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/laodiaoyadashu/logistic-regression-hw">戳我</a></p><span id="more"></span><h1 id="知识准备"><a href="#知识准备" class="headerlink" title="知识准备"></a>知识准备</h1><p>2019.11.24号这个周末用了两天的时间终于做完了，期间参考的各种资料我会在文章最后列出。那完成这次作业至少需要具备下面的知识点。</p><ol><li>微积分</li><li>梯度下降原理</li><li>pandas和numpy的基本使用</li></ol><h1 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h1><p>课程提供的作业要求<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.google.com/presentation/d/13LSyr3XV4ZrUYimgzzyLn8l50HphTQtvJ5n7XZZCwTw">文档</a>.(其中的kaggle提交地址已失效,我在文章末尾提供了训练集和测试集的下载地址)。给定训练集和测试集，根据训练集建立模型判断测试集中的每个id对应的收入是否超过50K.</p><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>首先，很明显的二分类问题，根据在b站(对，就是b站)<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.bilibili.com/video/av10590361?from=search&seid=12707508472476614247">课堂</a>上学到内容选择使用logistic regression(对数几率回归)来实现.数值优化使用梯度下降，当然其他方法也是可以的。</p><h2 id="1-引入对数几率预测函数"><a href="#1-引入对数几率预测函数" class="headerlink" title="1. 引入对数几率预测函数"></a>1. 引入对数几率预测函数</h2><p>先引入一个概念：广义线性模型。在西瓜书上的定义是这样的考虑单调可微函数$g(X)$令 $ y = g(w^Tx +b)$这样就得到了“广义线性模型(generalized linear model)“其中函数$g(X)$称为”联系函数(link function)“.显然，线性回归模型就是$g(X) = 1,x \in R$ 。 现在回到问题中，我们需要使用线性模型解决一个二分类的问题，那么只需要找到一个单调可微的函数$g(X)$将分类任务的真实标记y与线性回归模型的预测值联系起来即可。这句不理解没关系，接着看下一段就明白了</p><p>线性回归模型中产生的预测值$z = w^Tx + b$这里的$z$是一个实值$z \in R$。二分类问题中我们想要的预测值$y$是一个离散值0或者是1，即$y$的值是正例或者反例。那么我们只需要找一个函数 $ y = g(z) $ 它的输入值 $z$ 输出值是 $ y \in [0,1] $ 。最理想的函数是”单位阶跃函数“。如下：</p><p>$$g(z) = \begin{cases} 0 &amp; z &lt; 0\ 0.5&amp;z=0 \ 1&amp;z&gt;0 \end{cases} $$</p><p>即若预测值$z$大于0判断为分类1，小于0则判断我分类2，预测值为临界值则可以任意判别。但是这个函数如下图它并<strong>不连续不能作为广义线性模型中的$g(X)$。</strong>于是我们希望找到一个函数一定程度上相似单位阶跃函数同时单调可微。对数几率函数就是这样一个函数：</p><p>$$g(z) = \frac{1}{1 + e^{-z}}$$</p><p>对数几率函数与单位阶跃函数图像如下：</p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://imgchr.com/i/MjtvKH"><img src="https://s2.ax1x.com/2019/11/25/MjtvKH.md.png" alt="kanhui" style="zoom:67%"></a></p><p>我们把这个函数 代入到广义线性模型中就得到了对数几率回归的预测函数</p><p>$$y = \frac{1}{1+e^{-z}} = {1 \over 1+ e^{-{(w^Tx+b)}}}$$。</p><h3 id="1-1-解释对数几率的含义"><a href="#1-1-解释对数几率的含义" class="headerlink" title="1.1 解释对数几率的含义"></a>1.1 解释对数几率的含义</h3><p>现在我们来解释一下“对数几率”这个词的含义。首先我们先把 $y = \frac{1}{1+e^{-z}} $ 中的$z$提取出来就得到了 $ln{\frac{y}{1-y}}=w^Tx+b$ 。其中如果把y看作为样本是正例的可能性，则$1-y$是其反例的可能性，这两者的比值 $ \frac{y}{1-y} $就称为几率，反映了$x$作为正例的可能性。对几率取对数得到的 $ ln\frac{y}{1-y} $就称为对数几率。</p><h2 id="2-找到损失函数"><a href="#2-找到损失函数" class="headerlink" title="2.找到损失函数"></a>2.找到损失函数</h2><p>现在已经知道了对数几率回归的预测函数，只需要找到损失函数，然后用梯度下降法确定w和b的值就完成任务了。对数几率回归中损失函数并是线性回归中的平方误差损失，而是交叉熵(<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/tsyccnh/article/details/79163834">可以参考这篇文章</a>);或者也可以使用对数似然推导(可以参考书籍-机器学习-周志华第三章部分)出来损失函数结果是一样的。交叉熵老师在视频里已经给出了公式：</p><p>$$ L(w,b) = -[ylny^* + (1-y)ln(1-y^*)]$$</p><p>这个损失函数通常称作为 <strong>对数损失 (logloss)</strong>，这里的对数底为自然对数 <img src="https://www.zhihu.com/equation?tex=e" alt="[公式]"> ，其中真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 是有 0/1 两种情况，而推测值$ y^* $ 由于借助对数几率函数，其输出是介于0~1之间连续概率值。仔细查看，不难发现，当真实值 <img src="https://www.zhihu.com/equation?tex=y%3D0" alt="[公式]"> 时，第一项为0，当真实值 <img src="https://www.zhihu.com/equation?tex=y%3D1" alt="[公式]"> 时，第二项为0，所以，这个损失函数其实在每次计算时永远都只有一项在发挥作用，那这不就可以转换为分段函数了吗，分段的形式如下:</p><img src="https://s2.ax1x.com/2019/11/25/MvpeXR.png" alt="MvpeXR.png" border="0" style="zoom:50%"><p>不难发现，当真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 为1时，输出值 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="[公式]"> 越接近1，则 <img src="https://www.zhihu.com/equation?tex=L+" alt="[公式]"> 越小，当真实值 <img src="https://www.zhihu.com/equation?tex=y+" alt="[公式]"> 为 0 时，输出值 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="[公式]"> 越接近于0，则 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]"> 越小 (可自己手画一下 <img src="https://www.zhihu.com/equation?tex=-%5Clog%28x%29" alt="[公式]"> 函数的曲线)。该分段函数整合之后就是上面我们所列出的 logloss 损失函数的形式。</p><h2 id="3-使用梯度下降优化求解"><a href="#3-使用梯度下降优化求解" class="headerlink" title="3.使用梯度下降优化求解"></a>3.使用梯度下降优化求解</h2><p>我们确定了模型的损失函数，接下来就是根据这个损失函数，不断的优化模型的参数，从而获得拟合数据的最佳模型。损失函数如下：</p><p>$$L(w,b) = -[ylny^* + (1-y)ln(1-y^*)], 注意y不是自变量，y^*=\frac{1}{1+e^{-z}}, z = w^Tx + b$$</p><p>最小化损失值便得到</p><p>$(w^*,b^*) = argmin_{w,b}(L(w,b))$</p><p>梯度下降中的$(w,b)$的更新方式</p><p>$$w \leftarrow w - \alpha\frac{\partial L}{\partial w}$$</p><p>$$b \leftarrow b - \alpha\frac{\partial L}{\partial b}$$</p><p>由微积分的链式求导可以得到</p><p>$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y^*} \frac{\partial y^*}{\partial z} \frac{\partial z}{\partial w} = (y^* - y)x$$</p><p>$$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial y^*} \frac{\partial y^*}{\partial z} \frac{\partial z}{\partial b}=y^*-y$$</p><p>转化为矩阵运算：</p><p>为了简便运算我们在训练集上添加一个全为1的列，使偏置系数b更新的时候不受x值得影响，接着令$\beta = (W,b)$那么参数更新就可以写为</p><p>$$\beta = \beta - \alpha X^T(Y^* - Y)$$。</p><p>其中它们的shape如下：</p><p>$$\begin{cases} \beta &amp; (n,1) \ X &amp; (m,n) \ Y^*,Y&amp;(m,1)\end{cases}$$</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>下面是代码实现的部分，其中数据处理部分占了很大一部分，核心的梯度下降部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_secent</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        x_train: m x n</span></span><br><span class="line"><span class="string">        y_train: m x 1</span></span><br><span class="line"><span class="string">    return: wb</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    lr_rate, threshold = <span class="number">1</span>, <span class="number">0.01</span>	<span class="comment"># 初始化学习速度和阈值</span></span><br><span class="line">    wb = np.ones((x_train.shape[<span class="number">1</span>], <span class="number">1</span>))  <span class="comment"># n x 1, 这里的wb就是刚刚的贝塔（就是那个和B很像的字母）</span></span><br><span class="line">    g = np.zeros((x_train.shape[<span class="number">1</span>], <span class="number">1</span>))	<span class="comment"># 使用adagrad更新学习速率，</span></span><br><span class="line">    y_predict = get_y_predict(wb, x_train)</span><br><span class="line">    loss_value = get_loss_value(wb, y_train, y_predict)  <span class="comment"># 计算损失值</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        wb, g = update_parameter(wb, lr_rate, x_train, y_train, y_predict, g)</span><br><span class="line">        y_predict = get_y_predict(wb, x_train)</span><br><span class="line">        pre_loss_value, loss_value = loss_value, get_loss_value(</span><br><span class="line">            wb, y_train, y_predict)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(pre_loss_value - loss_value) &lt; threshold: <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> wb</span><br></pre></td></tr></table></figure><p>完整的代码部分如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_value</span>(<span class="params">column_data,cond</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        column_data: 需要二值化的列</span></span><br><span class="line"><span class="string">        cond: 二值化的条件，符合该条件的设置为1否则为0</span></span><br><span class="line"><span class="string">    return: 返回处理好的数据</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    column_data.mask(cond=cond,other=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    column_data.where(cond=column_data==<span class="number">1</span>,other=<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#print(column_data)</span></span><br><span class="line">    <span class="keyword">return</span> column_data.astype(<span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_scale</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    对数据进行one-hot编码，并进行缩放.</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        data: 原始数据</span></span><br><span class="line"><span class="string">    return: 处理后的数据</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    object_columns = [column <span class="keyword">for</span> column <span class="keyword">in</span> data.columns <span class="keyword">if</span> data[column].dtype == <span class="string">&#x27;object&#x27;</span>]</span><br><span class="line">    number_columns = [column <span class="keyword">for</span> column <span class="keyword">in</span> data.columns <span class="keyword">if</span> data[column].dtype == <span class="string">&#x27;int64&#x27;</span> <span class="keyword">and</span> column != <span class="string">&#x27;income&#x27;</span> <span class="keyword">and</span> column != <span class="string">&#x27;sex&#x27;</span>]</span><br><span class="line">    object_columns, number_columns = data[object_columns],data[number_columns]  <span class="comment"># 提取相应的列</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对数据进行标准化缩放,缩放后值在0-1之间</span></span><br><span class="line">    number_columns = (number_columns - number_columns.<span class="built_in">min</span>()) / (number_columns.<span class="built_in">max</span>() - number_columns.<span class="built_in">min</span>())</span><br><span class="line">    object_columns = pd.get_dummies(object_columns)</span><br><span class="line">    data = pd.concat([object_columns, number_columns, data[<span class="string">&#x27;sex&#x27;</span>], data[<span class="string">&#x27;income&#x27;</span>]],axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_pre_process</span>(<span class="params">df_data,df_test_data</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    1. 进行数据预处理</span></span><br><span class="line"><span class="string">    2. 返回训练集和验证集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    df_data,df_test_data = df_data.fillna(<span class="number">0</span>),df_test_data.fillna(<span class="number">0</span>)  <span class="comment"># 对空值进行填充</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对性别和income列处理</span></span><br><span class="line">    df_data[<span class="string">&#x27;sex&#x27;</span>] = binary_value(df_data[<span class="string">&#x27;sex&#x27;</span>].copy(deep=<span class="literal">True</span>),cond=df_data[<span class="string">&#x27;sex&#x27;</span>]==<span class="string">&#x27; Male&#x27;</span>)</span><br><span class="line">    df_test_data[<span class="string">&#x27;sex&#x27;</span>] = binary_value(df_test_data[<span class="string">&#x27;sex&#x27;</span>].copy(deep=<span class="literal">True</span>),cond=df_test_data[<span class="string">&#x27;sex&#x27;</span>]==<span class="string">&#x27; Male&#x27;</span>)</span><br><span class="line">    df_data[<span class="string">&#x27;income&#x27;</span>] = binary_value(df_data[<span class="string">&#x27;income&#x27;</span>].copy(deep=<span class="literal">True</span>),cond=df_data[<span class="string">&#x27;income&#x27;</span>]==<span class="string">&#x27; &gt;50K&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    df_data = one_hot_scale(df_data)</span><br><span class="line">    df_test_data= one_hot_scale(df_test_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把训练集分为训练集和验证集</span></span><br><span class="line">    x_train = df_data.iloc[:<span class="built_in">int</span>(<span class="built_in">len</span>(df_data.index) * <span class="number">0.80</span>), :]</span><br><span class="line">    x_valid = df_data.iloc[<span class="built_in">int</span>(<span class="built_in">len</span>(df_data.index) * <span class="number">0.80</span>):, :]</span><br><span class="line">    </span><br><span class="line">    y_train = np.array(x_train.iloc[:, -<span class="number">1</span>:])</span><br><span class="line">    x_train = np.array(x_train.iloc[:, :-<span class="number">1</span>])</span><br><span class="line">    x_train = np.c_[x_train, np.ones((x_train.shape[<span class="number">0</span>], <span class="number">1</span>))]  <span class="comment"># 添加一个横为1的列，用于简化偏置系数b的更新</span></span><br><span class="line">    y_valid = np.array(x_valid.iloc[:, -<span class="number">1</span>:])</span><br><span class="line">    x_valid = np.array(x_valid.iloc[:, :-<span class="number">1</span>])</span><br><span class="line">    x_valid = np.c_[x_valid, np.ones((x_valid.shape[<span class="number">0</span>], <span class="number">1</span>))]</span><br><span class="line">    </span><br><span class="line">    y_test = np.array(df_test_data.iloc[:,-<span class="number">1</span>:])</span><br><span class="line">    x_test = np.array(df_test_data.iloc[:,:-<span class="number">1</span>])</span><br><span class="line">    x_test = np.c_[x_test, np.ones((x_test.shape[<span class="number">0</span>], <span class="number">1</span>))]  <span class="comment"># 添加一个横为1的列，用于简化偏置系数b的更新</span></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_valid, y_valid,x_test,y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_secent</span>(<span class="params">x_train, y_train</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        x_train: m x n</span></span><br><span class="line"><span class="string">        y_train: m x 1</span></span><br><span class="line"><span class="string">    return: wb</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    lr_rate, threshold = <span class="number">1</span>, <span class="number">5</span></span><br><span class="line">    wb = np.ones((x_train.shape[<span class="number">1</span>], <span class="number">1</span>))  <span class="comment"># n x 1</span></span><br><span class="line">    g = np.zeros((x_train.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    y_predict = get_y_predict(wb, x_train)</span><br><span class="line">    loss_value = get_loss_value(wb, y_train, y_predict)  <span class="comment"># 计算损失值</span></span><br><span class="line">    loss_value_list = []</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        loss_value_list.append(loss_value)</span><br><span class="line">        wb, g = update_parameter(wb, lr_rate, x_train, y_train, y_predict, g)</span><br><span class="line">        y_predict = get_y_predict(wb, x_train)</span><br><span class="line">        pre_loss_value, loss_value = loss_value, get_loss_value(wb, y_train, y_predict)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(pre_loss_value - loss_value) &lt; threshold: <span class="keyword">break</span>  <span class="comment"># 循环结束条件</span></span><br><span class="line">    plot_loss_value(loss_value_list)    <span class="comment">#画图</span></span><br><span class="line">    <span class="keyword">return</span> wb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameter</span>(<span class="params">wb, lr_rate, x_train, y_train, y_predict, g</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        wb: n x 1</span></span><br><span class="line"><span class="string">        g: adagrad 里的 G, n x 1</span></span><br><span class="line"><span class="string">        lr_rate: 学习速率</span></span><br><span class="line"><span class="string">        y_train: 真实值，m x 1</span></span><br><span class="line"><span class="string">        y_predict: 预测值，m x 1</span></span><br><span class="line"><span class="string">    return: wb, g</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    descent = x_train.T.dot(y_predict - y_train)  <span class="comment"># n x 1</span></span><br><span class="line">    g = g + descent * descent</span><br><span class="line">    lr_rates = lr_rate / (np.sqrt(g + <span class="number">0.00000001</span>))  <span class="comment"># n x 1</span></span><br><span class="line">    wb = wb - lr_rates * descent</span><br><span class="line">    <span class="keyword">return</span> wb, g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_y_predict</span>(<span class="params">wb, x_train</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        wb: 参数，n x 1</span></span><br><span class="line"><span class="string">        x_train: 训练集，m x n</span></span><br><span class="line"><span class="string">    return:计算预测值 y.shape = m x 1</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    y = wb.T.dot(x_train.T)  <span class="comment"># 1 x n * n x m = 1 x m</span></span><br><span class="line">    y = <span class="number">1</span> / (np.exp(-<span class="number">1</span> * y) + <span class="number">1</span>)  <span class="comment"># 1 x m</span></span><br><span class="line">    y = y.T  <span class="comment"># m x 1</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_loss_value</span>(<span class="params">wb, y_train, y_predict</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        wb: 参数，n x 1</span></span><br><span class="line"><span class="string">        y_train: 真实值，m x 1</span></span><br><span class="line"><span class="string">        y_predict: 预测值，m x 1</span></span><br><span class="line"><span class="string">    return: loss_value, 损失值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    res = y_train.T.dot(np.log(y_predict))  <span class="comment"># 1 x m * m x 1</span></span><br><span class="line">    res = -<span class="number">1</span> * res</span><br><span class="line">    tmp = (y_train - <span class="number">1</span>).T.dot(np.log(<span class="number">1</span> - y_predict))</span><br><span class="line">    res = res + tmp</span><br><span class="line">    <span class="keyword">return</span> res[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_acc</span>(<span class="params">wb, x, y</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        wb: n x 1</span></span><br><span class="line"><span class="string">        x: m x n</span></span><br><span class="line"><span class="string">        y: m x 1</span></span><br><span class="line"><span class="string">    return: 正确率 acc</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    y_predict = get_y_predict(wb, x)  <span class="comment"># m x 1</span></span><br><span class="line">    y_predict[y_predict &gt; <span class="number">0.5</span>] = <span class="number">1</span></span><br><span class="line">    y_predict[y_predict &lt;= <span class="number">0.5</span>] = <span class="number">0</span></span><br><span class="line">    acc = <span class="built_in">sum</span>(y_predict == y)[<span class="number">0</span>] / y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_data_process</span>(<span class="params">df_data</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        df_data: m x n</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">        x_test, y_test</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    train_data = pd.read_csv(<span class="string">&#x27;./data/train.csv&#x27;</span>)</span><br><span class="line">    test_data = pd.read_csv(<span class="string">&#x27;./data/test.csv&#x27;</span>)</span><br><span class="line">    x_train, y_train, x_valid,y_valid,x_test,y_test = data_pre_process(train_data,test_data)</span><br><span class="line">    wb = gradient_secent(x_train, y_train)</span><br><span class="line">    valid_acc = get_acc(wb, x_valid, y_valid)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;valid accuracy:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(valid_acc))</span><br><span class="line">    test_acc = get_acc(wb,x_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;tet accuracy:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_acc))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_loss_value</span>(<span class="params">loss_values</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    画出loss value的变化图</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(loss_values)),loss_values,linewidth=<span class="number">4</span>,color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    李宏毅机器学习第二次作业</span></span><br><span class="line"><span class="string">    1. 对数几率回归</span></span><br><span class="line"><span class="string">    2. 使用梯度下降寻找最优解</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正在执行中，可能需要耗费大量时间，如果超过10分钟未响应请手动关掉即可。&quot;</span>)</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行截图：</p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://imgchr.com/i/MjjxZ8"><img src="https://s2.ax1x.com/2019/11/25/MjjxZ8.md.png" alt="MjjxZ8.md.png" style="zoom:50%"></a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://imgchr.com/i/MjvZZT"><img src="https://s2.ax1x.com/2019/11/25/MjvZZT.md.png" alt="MjvZZT.png" border="0" style="zoom:50%"></a></p><p>可以看出在验证集和测试集中正确率还算看的过去。。。可能做了特征工程后正确率会高一点。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>感谢下面的博主和作者。</p><ol><li><p>《机器学习》——周志华</p></li><li><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/36670444">对数几率回归 —— Logistic Regression</a></p></li><li><p>数据集<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/maplezzz/NTU_ML2017_Hung-yi-Lee_HW">连接</a></p></li></ol></div><div class="article-licensing box"><div class="licensing-title"><p>李宏毅机器学习logistic-regression</p><p><a href="https://www.huihuidehui.com/posts/2a54212a.html">https://www.huihuidehui.com/posts/2a54212a.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>lalaking</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-11-25</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2020-02-11</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener external nofollow noreferrer" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/eb623950.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">python-supper方法用法-在子类中调用父类方法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/d9568da.html"><span class="level-item">踩坑-python创建二维数组</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js"></script><script>new Valine({el:"#valine-thread",appId:"X6DQHGReyaU0YRYI51rPUckX-gzGzoHsz",appKey:"i4vQP7YY3tnckaVP7UhjYmMh",avatar:"mm",avatarForce:!1,meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",visitor:!1,highlight:!0,recordIP:!1,enableQQ:!1,requiredFields:[]})</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="辉辉的辉"></figure><p class="title is-size-4 is-block" style="line-height:inherit">辉辉的辉</p><p class="is-size-6 is-block">永远年轻，永远热泪盈眶</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>银河系</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lalaking666" target="_blank" rel="noopener external nofollow noreferrer">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener external nofollow noreferrer" title="Github" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#知识准备"><span class="level-left"><span class="level-item">1</span><span class="level-item">知识准备</span></span></a></li><li><a class="level is-mobile" href="#作业要求"><span class="level-left"><span class="level-item">2</span><span class="level-item">作业要求</span></span></a></li><li><a class="level is-mobile" href="#思路分析"><span class="level-left"><span class="level-item">3</span><span class="level-item">思路分析</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-引入对数几率预测函数"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">1. 引入对数几率预测函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-解释对数几率的含义"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">1.1 解释对数几率的含义</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-找到损失函数"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">2.找到损失函数</span></span></a></li><li><a class="level is-mobile" href="#3-使用梯度下降优化求解"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">3.使用梯度下降优化求解</span></span></a></li></ul></li><li><a class="level is-mobile" href="#代码实现"><span class="level-left"><span class="level-item">4</span><span class="level-item">代码实现</span></span></a></li><li><a class="level is-mobile" href="#参考链接"><span class="level-left"><span class="level-item">5</span><span class="level-item">参考链接</span></span></a></li></ul></div></div><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/JS%E9%80%86%E5%90%91/"><span class="level-start"><span class="level-item">JS逆向</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BD%92%E6%A1%A3/"><span class="level-start"><span class="level-item">归档</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%88%AC%E8%99%AB/"><span class="level-start"><span class="level-item">爬虫</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2025-04-25T01:56:36.000Z">2025-04-25</time></p><p class="title"><a href="/posts/a70d8ab5.html">某壳 wll-kgsa参数算法分析及还原</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2025-03-17T09:02:42.000Z">2025-03-17</time></p><p class="title"><a href="/posts/e474db30.html">极验-3代一键通过模式-反混淆fullpage.9.1.9.cyhomb.js</a></p><p class="categories"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-04-10T06:22:26.000Z">2023-04-10</time></p><p class="title"><a href="/posts/507c1bbe.html">某安X-App-Token参数逆向|基于unidbg模拟调用生成</a></p><p class="categories"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-04-10T02:05:18.000Z">2023-04-10</time></p><p class="title"><a href="/posts/a3dc8a98.html">Picgo搭建Github图床使用Jsdelivr的正确姿势</a></p><p class="categories"><a href="/categories/%E5%BD%92%E6%A1%A3/">归档</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2023-02-18T08:08:55.000Z">2023-02-18</time></p><p class="title"><a href="/posts/78f06c31.html">JS逆向1: 有道翻译sign参数还原以及api返回数据解密</a></p><p class="categories"><a href="/categories/JS%E9%80%86%E5%90%91/">JS逆向</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">四月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">三月 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">四月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">八月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AES/"><span class="tag">AES</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/APP%E9%80%86%E5%90%91/"><span class="tag">APP逆向</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MD5/"><span class="tag">MD5</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ast/"><span class="tag">ast</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/flask/"><span class="tag">flask</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mac%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"><span class="tag">mac使用技巧</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/picgo/"><span class="tag">picgo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/unidbg/"><span class="tag">unidbg</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E5%BA%8A/"><span class="tag">图床</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9E%81%E9%AA%8C/"><span class="tag">极验</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tag">爬虫</span><span class="tag">4</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/favicon.svg" alt="Lalaking sharing" height="28"></a><p class="is-size-7"><span>&copy; 2025 lalaking</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener external nofollow noreferrer">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener external nofollow noreferrer" title="Download on GitHub" href="https://github.com/lalaking666"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;" rel="external nofollow noreferrer"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load",()=>{window.cookieconsent.initialise({type:"info",theme:"edgeless",static:!1,position:"bottom-left",content:{message:"此网站使用Cookie来改善您的体验。",dismiss:"知道了！",allow:"允许使用Cookie",deny:"拒绝",link:"了解更多",policy:"Cookie政策",href:"https://www.cookiesandyou.com/"},palette:{popup:{background:"#edeff5",text:"#838391"},button:{background:"#4b81e8"}}})})</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><script type="text/javascript" id="MathJax-script" async>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},chtml:{matchFontHeight:!1}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;" rel="external nofollow noreferrer">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})}))</script></body></html>